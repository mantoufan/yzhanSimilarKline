import streamlit as st
import requests
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from datetime import datetime, timedelta
from scipy.stats import pearsonr
from scipy.spatial.distance import euclidean
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import jieba
import re
import adata
import os
from dotenv import load_dotenv
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional
from functools import wraps
import json

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# API é…ç½® - ä¼˜å…ˆä» .env è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä» streamlit ç¯å¢ƒå˜é‡è¯»å–
def get_config(key, default_value=""):
    """å®‰å…¨åœ°è·å–é…ç½®å€¼ï¼Œä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–ï¼Œç„¶åå°è¯•ä» secrets è·å–"""
    try:
        # é¦–å…ˆå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
        value = os.getenv(key)
        if value is not None:
            return value
            
        # å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œå°è¯•ä» secrets è·å–
        try:
            return st.secrets.get(key, default_value)
        except FileNotFoundError:
            return default_value
    except Exception:
        return default_value

# ä½¿ç”¨æ–°çš„é…ç½®è·å–å‡½æ•°
API_KEY = get_config('API_KEY')
API_BASE = get_config('API_BASE', "https://api.openai.com")
MODEL = get_config('MODEL', "gpt-4o-mini")
PROXY_URL = get_config('PROXY_URL')

def file_cache(cache_dir="./data_cache", expire_days=1):
    """
    æ–‡ä»¶ç¼“å­˜è£…é¥°å™¨ï¼Œå°†æ•°æ®å­˜å‚¨åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
    
    å‚æ•°ï¼š
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        expire_days: ç¼“å­˜è¿‡æœŸå¤©æ•°ï¼Œé»˜è®¤1å¤©
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            # åˆ›å»ºç¼“å­˜ç›®å½•
            os.makedirs(cache_dir, exist_ok=True)
            
            # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨å‡½æ•°åå’Œå‚æ•°ä½œä¸ºç¼“å­˜é”®
            cache_key = f"{func.__name__}_{str(args)}_{str(kwargs)}"
            cache_file = os.path.join(cache_dir, f"{cache_key}.json")
            meta_file = os.path.join(cache_dir, f"{cache_key}_meta.json")
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ
            if os.path.exists(cache_file) and os.path.exists(meta_file):
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                cache_time = datetime.strptime(meta['timestamp'], 
                                             '%Y-%m-%d %H:%M:%S')
                
                # å¦‚æœç¼“å­˜æœªè¿‡æœŸï¼Œç›´æ¥ä»æ–‡ä»¶åŠ è½½æ•°æ®
                if datetime.now() - cache_time < timedelta(days=expire_days):
                    try:
                        with open(cache_file, 'r') as f:
                            return json.load(f)
                    except Exception as e:
                        print(f"è¯»å–ç¼“å­˜æ–‡ä»¶å‡ºé”™: {str(e)}")
            
            # å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œé‡æ–°è·å–æ•°æ®
            results = func(*args, **kwargs)
            
            # ä¿å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶
            try:
                # ä¿å­˜æ•°æ®
                with open(cache_file, 'w') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                
                # ä¿å­˜å…ƒæ•°æ®
                meta = {
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'function': func.__name__,
                    'args': str(args),
                    'kwargs': str(kwargs)
                }
                with open(meta_file, 'w') as f:
                    json.dump(meta, f, ensure_ascii=False, indent=2)
                    
            except Exception as e:
                print(f"å†™å…¥ç¼“å­˜æ–‡ä»¶å‡ºé”™: {str(e)}")
            
            return results
        return wrapper
    return decorator

@file_cache(cache_dir="./securities_cache", expire_days=30)
def load_security_data(security_type: str) -> pd.DataFrame:
    """
    åŠ è½½è¯åˆ¸æ•°æ®ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶ç¼“å­˜
    
    å‚æ•°ï¼š
        security_type: è¯åˆ¸ç±»å‹ ('index', 'stock', 'etf')
    
    è¿”å›ï¼š
        pd.DataFrame: åŒ…å«è¯åˆ¸ä¿¡æ¯çš„æ•°æ®æ¡†
    """
    try:
        if security_type == 'index':
            return adata.stock.info.all_index_code()
        elif security_type == 'stock':
            return adata.stock.info.all_code()
        elif security_type == 'etf':
            return adata.fund.info.all_etf_exchange_traded_info()
        else:
            return pd.DataFrame()
    except Exception as e:
        print(f"åŠ è½½{security_type}æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return pd.DataFrame()

def preprocess_query(query: str) -> str:
    """é¢„å¤„ç†æœç´¢å…³é”®è¯"""
    query = re.sub(r'[^\w\s]', '', query)
    query = query.lower()
    query = ' '.join(query.split())
    return query

@file_cache(cache_dir="./search_cache", expire_days=30)
def search_single_type(query: str, security_type: str) -> List[Dict]:
    """
    åœ¨å•ä¸ªè¯åˆ¸ç±»å‹ä¸­æœç´¢
    
    Args:
        query: æœç´¢å…³é”®è¯
        security_type: è¯åˆ¸ç±»å‹
    
    Returns:
        List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
    """
    results = []
    df = load_security_data(security_type)
    
    if df.empty:
        return results
        
    try:
        # æ ¹æ®è¯åˆ¸ç±»å‹ç¡®å®šä»£ç å’Œåç§°åˆ—
        code_col = {
            'index': 'index_code',
            'stock': 'stock_code',
            'etf': 'fund_code'
        }.get(security_type)
        
        name_col = 'name' if security_type == 'index' else 'short_name'
        
        # åˆ›å»ºæœç´¢æ¡ä»¶
        code_match = df[code_col].str.contains(query, case=False, na=False)
        name_match = df[name_col].str.contains(query, case=False, na=False)
        
        # åº”ç”¨æœç´¢æ¡ä»¶
        matched_df = df[code_match | name_match]
        
        # æå–ç»“æœ
        for _, row in matched_df.iterrows():
            results.append({
                'code': row[code_col],
                'name': row[name_col],
                'type': security_type,
                'exchange': row.get('exchange', '')
            })
            
    except Exception as e:
        print(f"æœç´¢{security_type}æ—¶å‡ºé”™: {str(e)}")
    
    return results

@lru_cache(maxsize=2056)
def search_securities(query: str) -> List[Dict]:
    """
    æœç´¢è¯åˆ¸(æŒ‡æ•°ã€è‚¡ç¥¨)
    
    æŠ€æœ¯ç‰¹ç‚¹:
    1. ä½¿ç”¨ LRU ç¼“å­˜ä¼˜åŒ–æ•°æ®åŠ è½½
    2. å¤šçº¿ç¨‹å¹¶è¡Œæœç´¢æå‡æ€§èƒ½
    3. å…³é”®è¯é¢„å¤„ç†æé«˜åŒ¹é…å‡†ç¡®æ€§
    4. å¼‚å¸¸å¤„ç†ç¡®ä¿åŠŸèƒ½ç¨³å®šæ€§
    5. ç±»å‹æ³¨è§£å¢å¼ºä»£ç å¯è¯»æ€§
    
    Args:
        query: æœç´¢å…³é”®è¯(ä»£ç æˆ–åç§°)
    
    Returns:
        List[Dict]: æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«:
            - code: è¯åˆ¸ä»£ç 
            - name: è¯åˆ¸åç§°
            - type: è¯åˆ¸ç±»å‹
            - exchange: äº¤æ˜“æ‰€
    """
    if not query or len(query.strip()) == 0:
        return []
        
    # é¢„å¤„ç†æŸ¥è¯¢å…³é”®è¯
    query = preprocess_query(query)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæœç´¢ä¸åŒç±»å‹çš„è¯åˆ¸
    security_types = ['index', 'stock', 'etf']
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = [
            executor.submit(search_single_type, query, security_type)
            for security_type in security_types
        ]
        
        # æ”¶é›†æ‰€æœ‰ç»“æœ
        all_results = []
        for future in futures:
            try:
                results = future.result()
                all_results.extend(results)
            except Exception as e:
                print(f"è·å–æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
                
    # æŒ‰ç›¸å…³åº¦æ’åºç»“æœ
    all_results.sort(key=lambda x: (
        # å®Œå…¨åŒ¹é…ä»£ç çš„ä¼˜å…ˆçº§æœ€é«˜
        -int(x['code'].lower() == query),
        # å…¶æ¬¡æ˜¯åŒ…å«ä»£ç çš„
        -int(query in x['code'].lower()),
        # å†æ¬¡æ˜¯åŒ…å«åç§°çš„
        -int(query in x['name'].lower()),
        # æœ€åæŒ‰ä»£ç é•¿åº¦æ’åº
        len(x['code'])
    ))
    
    return all_results

def get_market_data(code, security_type, days=365*3):
    """è·å–å¸‚åœºæ•°æ®"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    df = None
    
    if security_type == 'stock':
        df = adata.stock.market.get_market(
            stock_code=code,
            start_date=start_date.strftime('%Y-%m-%d'),
            k_type=1,
            adjust_type=1
        )
    elif security_type == 'etf':
        df = adata.fund.market.get_market_etf(
            fund_code=code,
            k_type=1
        )
    elif security_type == 'index':
        df = adata.stock.market.get_market_index(
            index_code=code,
            start_date=start_date.strftime('%Y-%m-%d'),
            k_type=1
        )
    
    if df is not None and not df.empty:
        df['trade_date'] = pd.to_datetime(df['trade_date'])
        numeric_columns = ['open', 'high', 'low', 'close']
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=numeric_columns)
        return df
    
    return None

def normalize_window(window):
    """æ ‡å‡†åŒ–ä»·æ ¼åºåˆ—"""
    numeric_window = pd.to_numeric(window, errors='coerce')
    if numeric_window.isna().any():
        return None
    return (numeric_window - numeric_window.iloc[0]) / numeric_window.iloc[0] * 100

def calculate_similarity(window1, window2):
    """è®¡ç®—ä¸¤ä¸ªçª—å£ä¹‹é—´çš„ç›¸ä¼¼åº¦"""
    if len(window1) != len(window2):
        return 0
    
    norm1 = normalize_window(window1)
    norm2 = normalize_window(window2)
    
    if norm1 is None or norm2 is None:
        return 0
    
    try:
        corr, _ = pearsonr(norm1, norm2)
        dist = euclidean(norm1, norm2)
        normalized_dist = 1 / (1 + dist/len(window1))
        similarity = (corr + 1)/2 * 0.7 + normalized_dist * 0.3
        return similarity
    except:
        return 0

def find_similar_patterns(df, window_size=30, top_n=3):
    """
    æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å†å²æ¨¡å¼
    
    æŠ€æœ¯ç‚¹ï¼š
    - Kçº¿æ¨¡å¼è¯†åˆ«ï¼šé€šè¿‡æ»‘åŠ¨çª—å£æ–¹æ³•è¯†åˆ«å†å²Kçº¿å½¢æ€
    - æ¬§å‡ é‡Œå¾—è·ç¦»ï¼šè®¡ç®—Kçº¿åºåˆ—é—´çš„è·ç¦»ç›¸ä¼¼åº¦
    - çš®å°”é€Šç›¸å…³ç³»æ•°ï¼šè®¡ç®—èµ°åŠ¿çš„ç›¸å…³æ€§
    - æ—¶é—´åºåˆ—æ ‡å‡†åŒ–ï¼šå°†ä¸åŒæ—¶æœŸçš„ä»·æ ¼åºåˆ—æ ‡å‡†åŒ–ä»¥ä¾¿æ¯”è¾ƒ
    """
    if df is None or len(df) < window_size * 2:
        return []
    
    recent_window = df.tail(window_size)['close'].values
    recent_window_norm = (recent_window - recent_window[0]) / recent_window[0] * 100
    
    similar_patterns = []
    max_i = len(df) - (window_size * 2 + 7)
    
    # æ‰¹é‡è®¡ç®—æ‰€æœ‰çª—å£çš„æ ‡å‡†åŒ–æ•°æ®
    all_windows = np.array([
        df['close'].values[i:i+window_size]
        for i in range(max_i)
    ])
    all_windows_norm = np.array([
        (window - window[0]) / window[0] * 100
        for window in all_windows
    ])
    
    # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
    correlations = np.array([
        pearsonr(recent_window_norm, window_norm)[0]
        for window_norm in all_windows_norm
    ])
    
    # æ‰¹é‡è®¡ç®—æ¬§æ°è·ç¦»
    distances = np.array([
        euclidean(recent_window_norm, window_norm)
        for window_norm in all_windows_norm
    ])
    normalized_distances = 1 / (1 + distances/window_size)
    
    # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
    similarities = correlations * 0.7 + normalized_distances * 0.3
    
    # è·å–æœ€ç›¸ä¼¼çš„æ¨¡å¼
    top_indices = np.argsort(similarities)[-top_n:][::-1]
    
    for i in top_indices:
        similar_patterns.append({
            'start_date': df.iloc[i]['trade_date'],
            'end_date': df.iloc[i+window_size-1]['trade_date'],
            'pattern_data': df.iloc[i:i+window_size].copy(),
            'future_data': df.iloc[i+window_size:i+window_size+7].copy(),
            'similarity': similarities[i]
        })
    
    return similar_patterns

def analyze_future_trends(similar_patterns):
    """
    åˆ†æå†å²Kçº¿åç»­èµ°åŠ¿
    
    æŠ€æœ¯ç‚¹ï¼š
    - å†å²æ¨¡å¼åŒ¹é…ï¼šåŸºäºç›¸ä¼¼Kçº¿æ¨¡å¼çš„å†å²è¡¨ç°
    - ç»Ÿè®¡æ¦‚ç‡åˆ†æï¼šè®¡ç®—æ¶¨è·Œæ¦‚ç‡å’Œå¹…åº¦åˆ†å¸ƒ
    - è¶‹åŠ¿é¢„æµ‹å»ºæ¨¡ï¼šæ„å»ºæœªæ¥å¯èƒ½çš„èµ°åŠ¿é¢„æµ‹
    """
    if not similar_patterns:
        return None
        
    stats = {
        'up': {str(i): {'count': 0, 'max': 0, 'min': float('inf'), 'mean': 0, 'values': []} 
               for i in range(1, 8)},
        'down': {str(i): {'count': 0, 'max': 0, 'min': float('inf'), 'mean': 0, 'values': []} 
                 for i in range(1, 8)}
    }
    
    for pattern in similar_patterns:
        future_data = pattern['future_data']
        
        for i in range(len(future_data)):
            day = str(i + 1)
            current_price = future_data.iloc[i]['close']
            prev_price = pattern['pattern_data'].iloc[-1]['close'] if i == 0 else future_data.iloc[i-1]['close']
            
            change_rate = ((current_price - prev_price) / prev_price) * 100
            
            category = 'up' if change_rate >= 0 else 'down'
            change_rate = abs(change_rate)
            
            stats[category][day]['count'] += 1
            stats[category][day]['values'].append(change_rate)
            stats[category][day]['max'] = max(stats[category][day]['max'], change_rate)
            stats[category][day]['min'] = min(stats[category][day]['min'], change_rate)
    
    total_patterns = len(similar_patterns)
    for category in ['up', 'down']:
        for day in stats[category]:
            day_stats = stats[category][day]
            if day_stats['count'] > 0:
                day_stats['probability'] = day_stats['count'] / total_patterns
                day_stats['mean'] = sum(day_stats['values']) / day_stats['count']
                rounded_values = [round(x, 2) for x in day_stats['values']]
                value_counts = {}
                for v in rounded_values:
                    value_counts[v] = value_counts.get(v, 0) + 1
                day_stats['mode'] = max(value_counts.items(), key=lambda x: x[1])[0]
                
                if day_stats['min'] == float('inf'):
                    day_stats['min'] = 0
                
            del day_stats['values']
    
    return stats

def analyze_holding_returns(similar_patterns):
    """
    åˆ†æä¸åŒæŒæœ‰æœŸçš„æ”¶ç›Šæƒ…å†µ
    
    æŠ€æœ¯ç‚¹ï¼š
    - æŒä»“æœŸæ”¶ç›Šç‡è®¡ç®—ï¼šè®¡ç®—ä¸åŒæŒæœ‰æœŸçš„æ”¶ç›Šè¡¨ç°
    - é£é™©åº¦é‡(æ ‡å‡†å·®/æ³¢åŠ¨ç‡)ï¼šè¯„ä¼°æŠ•èµ„é£é™©æ°´å¹³
    - èƒœç‡ç»Ÿè®¡ï¼šåˆ†æä¸åŒæŒæœ‰æœŸçš„ç›ˆåˆ©æ¦‚ç‡
    """
    if not similar_patterns:
        return None
        
    stats = {str(i): {
        'returns': [],
        'max_prices': [],
        'min_prices': [],
        'win_count': 0,
        'loss_count': 0,
    } for i in range(1, 8)}
    
    for pattern in similar_patterns:
        entry_price = pattern['pattern_data'].iloc[-1]['close']
        future_data = pattern['future_data']
        
        for days in range(1, 8):
            day_key = str(days)
            if days <= len(future_data):
                holding_period_data = future_data.iloc[:days]
                exit_price = holding_period_data.iloc[-1]['close']
                returns = (exit_price - entry_price) / entry_price * 100
                
                stats[day_key]['returns'].append(returns)
                
                if returns > 0:
                    stats[day_key]['win_count'] += 1
                else:
                    stats[day_key]['loss_count'] += 1
                
                stats[day_key]['max_prices'].append(holding_period_data['high'].max())
                stats[day_key]['min_prices'].append(holding_period_data['low'].min())
    
    analysis_results = {}
    for days, day_stats in stats.items():
        returns_array = np.array(day_stats['returns'])
        total_trades = len(returns_array)
        
        if total_trades > 0:
            analysis_results[days] = {
                'avg_return': np.mean(returns_array),
                'max_return': np.max(returns_array),
                'min_return': np.min(returns_array),
                'std_return': np.std(returns_array),
                'win_rate': day_stats['win_count'] / total_trades,
                'trade_count': total_trades,
                'max_price_change': (np.max(day_stats['max_prices']) - entry_price) / entry_price * 100,
                'min_price_change': (np.min(day_stats['min_prices']) - entry_price) / entry_price * 100
            }
    
    return analysis_results

class ChineseTextVectorizer:
    """ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–å™¨"""
    
    def __init__(self, vector_size=100):
        self.vector_size = vector_size
        self.tfidf = TfidfVectorizer(
            tokenizer=self._parallel_tokenize,  # ä½¿ç”¨å¹¶è¡Œåˆ†è¯
            token_pattern=None,
            max_features=2000  # å‡å°‘ç‰¹å¾æ•°é‡
        )
        self.svd = TruncatedSVD(
            n_components=vector_size,
            random_state=42,
            algorithm='randomized'  # ä½¿ç”¨éšæœºç®—æ³•åŠ é€Ÿ
        )
        self.is_fitted = False

         # é¢„åŠ è½½ç»“å·´è¯å…¸
        jieba.initialize()
    
    @lru_cache(maxsize=1000)  # ç¼“å­˜åˆ†è¯ç»“æœ
    def _tokenize(self, text):
        text = re.sub(r'[^\w\s]', '', text)
        words = jieba.lcut(text)
        return [w for w in words if w.strip()]
    
    def fit(self, texts):
        tfidf_matrix = self.tfidf.fit_transform(texts)
        self.svd.fit(tfidf_matrix)
        self.is_fitted = True
    
    def transform(self, text):
        tfidf_vector = self.tfidf.transform([text])
        vector = self.svd.transform(tfidf_vector)
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        return vector.flatten()
    
    def _parallel_tokenize(self, text):
        # å¯¹è¾ƒé•¿çš„æ–‡æœ¬è¿›è¡Œåˆ†å—
        if len(text) < 1000:  # çŸ­æ–‡æœ¬ç›´æ¥å¤„ç†
            return self._tokenize(text)
            
        # é•¿æ–‡æœ¬å¹¶è¡Œå¤„ç†
        chunks = self._split_text(text)
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(self._tokenize, chunks))
        return [token for chunk_result in results for token in chunk_result]

def call_llm_api(prompt):
    """è°ƒç”¨ LLM API"""
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }
    
    try:
        response = requests.post(
            f"{API_BASE}/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        st.error(f"API è°ƒç”¨å‡ºé”™: {str(e)}")
        return None

def compute_embedding(text, vectorizer=None):
    """è®¡ç®—æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
    if vectorizer is None:
        vectorizer = ChineseTextVectorizer()
        
    try:
        if not text.strip():
            return np.zeros(vectorizer.vector_size)
            
        if not vectorizer.is_fitted:
            vectorizer.fit([text])
            
        embedding = vectorizer.transform(text)
        return embedding
        
    except Exception as e:
        print(f"è®¡ç®—æ–‡æœ¬å‘é‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return np.zeros(vectorizer.vector_size)

def compute_similarity(query_embedding, chunk_embedding):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if query_embedding.shape != chunk_embedding.shape:
        raise ValueError("å‘é‡ç»´åº¦ä¸åŒ¹é…")
        
    similarity = np.dot(query_embedding, chunk_embedding)
    return similarity

def retrieve_relevant_chunks(query, chunks, top_k=3):
    """æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æœ¬å—"""
    vectorizer = ChineseTextVectorizer(vector_size=50)  # å‡å°‘å‘é‡ç»´åº¦
    
    # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
    all_texts = [query] + [chunk_text for _, chunk_text in chunks]
    vectorizer.fit(all_texts)
    
    # æ‰¹é‡è®¡ç®—å‘é‡
    query_embedding = compute_embedding(query, vectorizer)
    chunk_embeddings = np.vstack([
        compute_embedding(chunk_text, vectorizer)
        for _, chunk_text in chunks
    ])
    
    # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
    similarities = np.dot(chunk_embeddings, query_embedding)
    
    # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„chunk
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    return [chunks[i] for i in top_indices]

def create_text_chunks(security, current_df, similar_patterns, holding_stats):
    """
    å°†å¸‚åœºæ•°æ®åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„æ–‡æœ¬å—ï¼Œä¾¿äºåç»­æ£€ç´¢

    æŠ€æœ¯ç‚¹ï¼š
    - RAGæ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæ„å»ºç»“æ„åŒ–çš„æ–‡æœ¬å—ä¾›æ£€ç´¢
    - å‘é‡åŒ–(TF-IDF + SVD)ï¼šå°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡è¡¨ç¤º
    - ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…ï¼šè®¡ç®—æŸ¥è¯¢ä¸æ–‡æœ¬å—çš„ç›¸ä¼¼åº¦
    
    è¿™ä¸ªå‡½æ•°å°†å„ç±»å¸‚åœºæ•°æ®è½¬æ¢æˆç»“æ„åŒ–çš„æ–‡æœ¬å—ï¼ŒåŒ…æ‹¬ï¼š
    1. è¯åˆ¸åŸºæœ¬ä¿¡æ¯ï¼ˆä»£ç ã€åç§°ç­‰ï¼‰
    2. æœ€æ–°è¡Œæƒ…æ•°æ®
    3. è¿‘æœŸä»·æ ¼èµ°åŠ¿
    4. å†å²è¡¨ç°åˆ†æ
    5. ç›¸ä¼¼Kçº¿åˆ†æ
    6. æŒä»“æ”¶ç›Šåˆ†æ
    """
    chunks = []
    
    # æ„å»ºåŸºæœ¬ä¿¡æ¯æ–‡æœ¬å—
    basic_info = f"""
            è¯åˆ¸åŸºæœ¬ä¿¡æ¯ï¼š
            åç§°ï¼š{security['name']}
            ä»£ç ï¼š{security['code']}
            ç±»å‹ï¼š{security['type']}
            äº¤æ˜“æ‰€ï¼š{security['exchange'] if security['exchange'] else 'æœªçŸ¥'}
    """
    chunks.append(("basic_info", basic_info))
    
    if current_df is not None and not current_df.empty:
        # æ·»åŠ æœ€æ–°è¡Œæƒ…ä¿¡æ¯
        latest_data = current_df.iloc[-1]
        latest_market = f"""
            æœ€æ–°å¸‚åœºè¡Œæƒ…ï¼ˆ{latest_data['trade_date'].strftime('%Y-%m-%d')}ï¼‰ï¼š
            æ”¶ç›˜ä»·ï¼š{latest_data['close']:.2f}
            å¼€ç›˜ä»·ï¼š{latest_data['open']:.2f}
            æœ€é«˜ä»·ï¼š{latest_data['high']:.2f}
            æœ€ä½ä»·ï¼š{latest_data['low']:.2f}
            æˆäº¤é‡ï¼š{latest_data.get('volume', 'æœªçŸ¥')}
        """
        chunks.append(("latest_market", latest_market))
        
        # æ·»åŠ è¿‘æœŸèµ°åŠ¿åˆ†æï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼‰
        if len(current_df) >= 21:
            recent_trend = f"""
            è¿‘æœŸä»·æ ¼èµ°åŠ¿ï¼š
            5æ—¥æ¶¨è·Œå¹…ï¼š{((latest_data['close'] - current_df.iloc[-6]['close'])/current_df.iloc[-6]['close']*100):.2f}%
            10æ—¥æ¶¨è·Œå¹…ï¼š{((latest_data['close'] - current_df.iloc[-11]['close'])/current_df.iloc[-11]['close']*100):.2f}%
            20æ—¥æ¶¨è·Œå¹…ï¼š{((latest_data['close'] - current_df.iloc[-21]['close'])/current_df.iloc[-21]['close']*100):.2f}%
            """
            chunks.append(("recent_trend", recent_trend))
        
        # æ·»åŠ å†å²è¡¨ç°åˆ†æï¼ˆæœ€è¿‘3å¹´ï¼‰
        three_year_data = current_df.tail(252 * 3).copy()
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col in three_year_data.columns:
                three_year_data[col] = pd.to_numeric(three_year_data[col], errors='coerce')
        
        if not three_year_data.empty:
            # æŒ‰å¹´åº¦ç»Ÿè®¡å¸‚åœºè¡¨ç°
            yearly_data = {}
            for year in three_year_data['trade_date'].dt.year.unique():
                year_df = three_year_data[three_year_data['trade_date'].dt.year == year]
                if not year_df.empty:
                    yearly_data[year] = {
                        'start_price': year_df.iloc[0]['close'],
                        'end_price': year_df.iloc[-1]['close'],
                        'highest': year_df['high'].max(),
                        'lowest': year_df['low'].min(),
                        'vol_mean': pd.to_numeric(year_df.get('volume', pd.Series()), errors='coerce').mean()
                    }
            
            # æ„å»ºå¹´åº¦åˆ†ææ–‡æœ¬
            historical_trend = """
            è¿‘å¹´å¸‚åœºè¡¨ç°ï¼š"""
            for year, data in sorted(yearly_data.items(), reverse=True):
                yearly_return = ((data['end_price'] - data['start_price']) / data['start_price'] * 100)
                yearly_volatility = ((data['highest'] - data['lowest']) / data['lowest'] * 100)
                
                historical_trend += f"""
                {year}å¹´åº¦è¡¨ç°ï¼š
                å¹´åº¦æ¶¨è·Œå¹…ï¼š{yearly_return:.2f}%
                æœ€é«˜ä»·ï¼š{data['highest']:.2f}
                æœ€ä½ä»·ï¼š{data['lowest']:.2f}
                æ³¢åŠ¨ç‡ï¼š{yearly_volatility:.2f}%"""
                if pd.notnull(data['vol_mean']):
                    historical_trend += f"""å¹³å‡æˆäº¤é‡ï¼š{data['vol_mean']:.0f}"""
            
            # æ·»åŠ 3å¹´æ•´ä½“ç»Ÿè®¡
            total_return = ((latest_data['close'] - three_year_data.iloc[0]['close']) 
                          / three_year_data.iloc[0]['close'] * 100)
            max_price = three_year_data['high'].max()
            min_price = three_year_data['low'].min()
            total_volatility = ((max_price - min_price) / min_price * 100)
            
            historical_trend += f"""
            è¿‘å¹´æ•´ä½“ç»Ÿè®¡ï¼š
            ç´¯è®¡æ¶¨è·Œå¹…ï¼š{total_return:.2f}%
            å†å²æœ€é«˜ä»·ï¼š{max_price:.2f}
            å†å²æœ€ä½ä»·ï¼š{min_price:.2f}
            ä»·æ ¼æ³¢åŠ¨ç‡ï¼š{total_volatility:.2f}%
            """
            
            chunks.append(("historical_trend", historical_trend))
    
    # æ·»åŠ ç›¸ä¼¼Kçº¿åˆ†æç»“æœ
    if similar_patterns:
        for i, pattern in enumerate(similar_patterns[:3], 1):
            future_trend = ""
            for j, row in pattern['future_data'].iterrows():
                future_trend += f"""
            ç¬¬{j+1}æ—¥:{row["close"]:.2f}"""
            
            pattern_info = f"""
            å†å²ç›¸ä¼¼Kçº¿åˆ†æï¼ˆTOP {i}ï¼‰ï¼š
            ç›¸ä¼¼åº¦ï¼š{pattern['similarity']:.2%}
            å†å²æ—¶é—´æ®µï¼š{pattern['start_date'].strftime('%Y-%m-%d')} è‡³ {pattern['end_date'].strftime('%Y-%m-%d')}
            åç»­7æ—¥å®é™…èµ°åŠ¿ï¼š{future_trend}
                """
            chunks.append((f"similar_pattern_{i}", pattern_info))
    
    # æ·»åŠ æŒä»“åˆ†ææ•°æ®
    if holding_stats:
        for days, stats in holding_stats.items():
            holding_info = f"""            æŒä»“{days}å¤©åˆ†æï¼š
            å¹³å‡æ”¶ç›Šï¼š{stats['avg_return']:.2f}%
            èƒœç‡ï¼š{stats['win_rate']*100:.1f}%
            æœ€å¤§ä¸Šæ¶¨ï¼š{stats['max_return']:.2f}%
            æœ€å¤§ä¸‹è·Œï¼š{stats['min_return']:.2f}%
            æ³¢åŠ¨ç‡ï¼š{stats['std_return']:.2f}%"""
            chunks.append((f"holding_days_{days}", holding_info))
    
    return chunks

def get_analysis_prompt(query, relevant_chunks, chat_history=None):
    """
    æ„å»ºå¸¦æœ‰æ£€ç´¢ä¸Šä¸‹æ–‡å’Œå¯¹è¯å†å²çš„åˆ†ææç¤º
    
    Args:
        query: å½“å‰ç”¨æˆ·é—®é¢˜
        relevant_chunks: ç›¸å…³çš„å¸‚åœºæ•°æ®å—
        chat_history: ä¹‹å‰çš„å¯¹è¯å†å²è®°å½•ï¼Œæ ¼å¼ä¸º [(user_msg, assistant_msg), ...]
    """
    context = "\n".join([chunk for _, chunk in relevant_chunks])
    
    # æ„å»ºæç¤ºçš„åŸºç¡€éƒ¨åˆ†
    prompt = f"""ä½œä¸ºä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹ç›¸å…³å¸‚åœºæ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. åªä½¿ç”¨æä¾›çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦æ·»åŠ å…¶ä»–å¸‚åœºä¿¡æ¯
2. æ˜ç¡®åŒºåˆ†æ•°æ®æ”¯æŒçš„ç»“è®ºå’Œä¸ç¡®å®šçš„æ¨æµ‹
3. å¦‚æœæ•°æ®ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
4. é€‚å½“æé†’æŠ•èµ„é£é™©

ç›¸å…³å¸‚åœºæ•°æ®ï¼š
{context}
"""

    # å¦‚æœå­˜åœ¨å¯¹è¯å†å²ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
    if chat_history and len(chat_history) > 0:
        prompt += "\nå¯¹è¯å†å²ï¼š\n"
        for i, (user_msg, assistant_msg) in enumerate(chat_history, 1):
            prompt += f"ç¬¬{i}è½®é—®ç­”ï¼š\n"
            prompt += f"ç”¨æˆ·ï¼š{user_msg}\n"
            prompt += f"åŠ©æ‰‹ï¼š{assistant_msg}\n"
    
    # æ·»åŠ å½“å‰é—®é¢˜
    prompt += f"\nå½“å‰ç”¨æˆ·é—®é¢˜ï¼š{query}"
    
    # æ·»åŠ è§’è‰²æŒ‡ç¤º
    prompt += """

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å’Œå¯¹è¯å†å²ï¼Œéµå¾ªä»¥ä¸‹åŸåˆ™å›ç­”ï¼š
1. ç”¨ä¸“ä¸šä¸”é€šä¿—çš„è¯­è¨€å›ç­”é—®é¢˜ï¼Œç¡®ä¿åˆ†æé€»è¾‘æ¸…æ™°
2. å¦‚æœæ¶‰åŠåˆ°ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·ä¿æŒåˆ†æçš„è¿è´¯æ€§
3. åœ¨å›ç­”ä¸­é€‚å½“æä¾›ä¸€äº›æ€è€ƒçš„åˆ‡å…¥ç‚¹ï¼Œå¼•å¯¼ç”¨æˆ·è¿›è¡Œæ›´æ·±å…¥çš„æé—®
4. å¦‚æœç”¨æˆ·è¿½é—®æŸä¸ªè§‚ç‚¹ï¼Œè¯·è¿›ä¸€æ­¥å±•å¼€è§£é‡ŠèƒŒåçš„åŸç†å’Œä¾æ®
5. å¦‚æœæŸä¸ªåˆ†ææ¶‰åŠåˆ°å¤šä¸ªæ–¹é¢ï¼Œå¯ä»¥æ˜ç¡®æŒ‡å‡ºï¼Œæ–¹ä¾¿ç”¨æˆ·é€‰æ‹©æ„Ÿå…´è¶£çš„æ–¹å‘ç»§ç»­æ¢è®¨
"""
    
    return prompt

def display_market_analysis(current_df, similar_patterns, future_dates=None):
    """
    å¸‚åœºåˆ†æä¸»å‡½æ•°ï¼Œé›†æˆKçº¿å±•ç¤ºã€æŠ€æœ¯åˆ†æå’Œæ™ºèƒ½é—®ç­”åŠŸèƒ½
    """
    if current_df is None or current_df.empty:
        st.warning("æ— æ³•è·å–å¸‚åœºæ•°æ®")
        return None
        
    # æ˜¾ç¤ºå½“å‰Kçº¿å›¾
    with st.container():
        st.markdown("### å½“å‰Kçº¿å›¾")
        if not current_df.empty:
            fig = plot_kline(current_df, "", future_dates=future_dates)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.write("æ— æ³•ç”ŸæˆKçº¿å›¾")
    
    # åˆ†æå¹¶æ˜¾ç¤ºç›¸ä¼¼Kçº¿
    if similar_patterns: 
        st.markdown("### å†å²ç›¸ä¼¼Kçº¿å›¾ï¼ˆå±•ç¤ºç›¸ä¼¼åº¦æœ€é«˜çš„å‰3æ¡ï¼‰")
        
        # æ˜¾ç¤ºæ¯ä¸ªç›¸ä¼¼æ¨¡å¼
        for i, pattern in enumerate(similar_patterns[:3], 1):
            with st.container():
                st.markdown(f"#### TOP {i} ç›¸ä¼¼åº¦: {pattern['similarity']:.2%}")
                st.markdown(f"å†å²æ—¶é—´æ®µ: {pattern['start_date'].strftime('%Y-%m-%d')} - {pattern['end_date'].strftime('%Y-%m-%d')}")
                
                fig_similar = plot_kline(
                    pattern['pattern_data'],
                    "",
                    future_df=pattern['future_data'],
                    show_future_data=True
                )
                st.plotly_chart(fig_similar, use_container_width=True)
        
        # æ˜¾ç¤ºè¶‹åŠ¿åˆ†æ
        display_trend_analysis(similar_patterns)

        # è®¡ç®—å’Œæ˜¾ç¤ºæŒä»“æ”¶ç›Šåˆ†æ
        holding_stats = analyze_holding_returns(similar_patterns)
        display_holding_period_analysis(holding_stats)
        
        return holding_stats
    
    return None

def display_rag_qa(security, current_df, similar_patterns, holding_stats):
    """
    æ˜¾ç¤ºæ”¯æŒå¤šè½®å¯¹è¯çš„æ™ºèƒ½é—®ç­”ç•Œé¢
    """
    st.markdown("""
### æ™ºèƒ½é—®ç­”åŠ©æ‰‹
<div style='background-color: #eef6ff; padding: 12px; border-radius: 8px; margin-bottom: 20px; font-size: 0.9em;'>
    ğŸ“ <strong>ä½¿ç”¨æŒ‡å—</strong>
    <ul style='margin-top: 8px; margin-bottom: 8px;'>
        <li>è¿™æ˜¯ä¸€ä¸ªæ”¯æŒå¤šè½®å¯¹è¯çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ‚¨å¯ä»¥å›´ç»•ä¸€ä¸ªè¯é¢˜æ·±å…¥äº¤æµ</li>
        <li>åŠ©æ‰‹ä¼šè®°ä½å¯¹è¯å†…å®¹ï¼Œæ‚¨å¯ä»¥åŸºäºä¹‹å‰çš„å›ç­”ç»§ç»­æé—®</li>
        <li>éšæ—¶å¯ä»¥è¦æ±‚åŠ©æ‰‹è§£é‡ŠæŸä¸ªè§‚ç‚¹ï¼Œæˆ–è€…æä¾›æ›´è¯¦ç»†çš„åˆ†æ</li>
        <li>å¦‚æœåˆ†æä¸å¤Ÿæ¸…æ™°ï¼Œè¯·å‘Šè¯‰åŠ©æ‰‹â€œå…·ä½“è¯´æ˜ä¸€ä¸‹å—ï¼Ÿâ€/li>
    </ul>
</div>
""", unsafe_allow_html=True)

    # åˆ›å»ºæ–‡æœ¬å—ç”¨äºæ£€ç´¢
    chunks = create_text_chunks(security, current_df, similar_patterns, holding_stats)
    base_key = f"{security['code']}_{security['type']}"
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    # æ˜¾ç¤ºå†å²å¯¹è¯
    for i, (user_msg, assistant_msg) in enumerate(st.session_state.chat_history):
        with st.container():
            # ç”¨æˆ·æ¶ˆæ¯
            st.markdown(f"""
            <div style='background-color: #f0f2f6; padding: 10px; border-radius: 8px; margin-bottom: 10px;'>
                <span style='color: #666;'>ğŸ‘¤ æ‚¨ï¼š</span><br>
                {user_msg}
            </div>
            """, unsafe_allow_html=True)
            
            # åŠ©æ‰‹å›å¤
            st.markdown(f"""
            <div style='background-color: #e8f4f9; padding: 10px; border-radius: 8px; margin-bottom: 20px;'>
                <span style='color: #666;'>ğŸ¤– åŠ©æ‰‹ï¼š</span><br>
                {assistant_msg}
            </div>
            """, unsafe_allow_html=True)
    
    # ç”¨æˆ·è¾“å…¥æ–°é—®é¢˜
    user_question = st.text_input(
        f"è¯·è¾“å…¥æ‚¨å…³äº {security['name']}ï¼ˆ{security['code']}ï¼‰çš„é—®é¢˜ï¼š", 
        key=f'rag_question_{base_key}',
        help="æ‚¨å¯ä»¥ï¼š\n1. è¯¢é—®åŸºæœ¬ä¿¡æ¯ã€æœ€æ–°è¡Œæƒ…ã€å†å²è¡¨ç°ã€ç›¸ä¼¼Kçº¿åˆ†æç­‰å†…å®¹\n2. åŸºäºåŠ©æ‰‹çš„å›ç­”ç»§ç»­è¿½é—®ï¼Œæ¯”å¦‚'ä¸ºä»€ä¹ˆä¼šè¿™æ ·é¢„æµ‹ï¼Ÿ'\n3. å¯»æ±‚æ›´è¯¦ç»†çš„è§£é‡Šï¼Œå¦‚'èƒ½å…·ä½“è§£é‡Šä¸€ä¸‹è¿™ä¸ªåŸå› å—ï¼Ÿ'\n4. è¦æ±‚å±•å¼€æŸä¸ªè§‚ç‚¹ï¼Œå¦‚'åˆšæ‰è¯´åˆ°xxxï¼Œèƒ½è¯¦ç»†åˆ†æä¸€ä¸‹å—ï¼Ÿ'"
    )
    
    if user_question:
        with st.spinner('æ­£åœ¨æ ¹æ®ç›¸å…³æ•°æ®å—æ€è€ƒç­”æ¡ˆ...'):
            # æ£€ç´¢ç›¸å…³æ–‡æœ¬å—
            relevant_chunks = retrieve_relevant_chunks(user_question, chunks, top_k=10)
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æœ¬å—
            chunks_content = ""
            for _, chunk_text in relevant_chunks:
                chunks_content += f'<p>{chunk_text}</p>'

            st.markdown(f'<details class="details" style="margin-top: -6px;">'
                    f'<summary class="details-summary">ğŸ” ç›¸å…³æ•°æ®å—ï¼ˆåŸºäº RAG ç»“æœï¼‰</summary>'
                    f'<div class="details-body details-body-related-chunks">'
                    f'{chunks_content}'
                    f'</div>'
                    f'</details>', unsafe_allow_html=True)
            
            # æ„å»ºpromptå¹¶è°ƒç”¨APIï¼ŒåŠ å…¥å¯¹è¯å†å²
            prompt = get_analysis_prompt(
                user_question, 
                relevant_chunks, 
                st.session_state.chat_history
            )
            response = call_llm_api(prompt)
            
            if response:
                # ä¿å­˜æ–°çš„å¯¹è¯è®°å½•
                st.session_state.chat_history.append((user_question, response))
                
                # æ˜¾ç¤ºæœ€æ–°çš„å›å¤
                st.markdown("""
                <div style='background-color: #f0f2f6; padding: 10px; border-radius: 8px; margin-bottom: 10px;'>
                    <span style='color: #666;'>ğŸ‘¤ æ‚¨ï¼š</span><br>
                    {user_question}
                </div>
                """.format(user_question=user_question), unsafe_allow_html=True)
                
                st.markdown("""
                <div style='background-color: #e8f4f9; padding: 10px; border-radius: 8px; margin-bottom: 20px;'>
                    <span style='color: #666;'>ğŸ¤– åŠ©æ‰‹ï¼š</span><br>
                    {response}
                </div>
                """.format(response=response), unsafe_allow_html=True)
                
                # æ·»åŠ å…è´£å£°æ˜
                st.markdown("""
                <div class="statement">
                âš ï¸ <strong>å…è´£å£°æ˜ï¼š</strong>ä»¥ä¸Šåˆ†æä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚
                </div>
                """, unsafe_allow_html=True)
                
                # æ¸…ç©ºè¾“å…¥æ¡†
                st.text_input(
                    f"è¯·è¾“å…¥æ‚¨å…³äº {security['name']}ï¼ˆ{security['code']}ï¼‰çš„é—®é¢˜æˆ–ç»§ç»­æé—®ï¼š",
                    value="",
                    key=f'rag_question_{base_key}_new',
                    help="æ‚¨å¯ä»¥è¯¢é—®å…³äºè¯¥è¯åˆ¸çš„åŸºæœ¬ä¿¡æ¯ã€æœ€æ–°è¡Œæƒ…ã€å†å²è¡¨ç°ã€ç›¸ä¼¼Kçº¿åˆ†æç­‰é—®é¢˜ã€‚æ‚¨ä¹Ÿå¯ä»¥åŸºäºä¹‹å‰çš„å¯¹è¯ç»§ç»­æé—®ã€‚"
                )
            else:
                st.error("æŠ±æ­‰ï¼Œè·å–å›ç­”å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚")

def main():
    """
    ä¸»å‡½æ•°ï¼šå®ç°é‡‘èæ•°æ®åˆ†æç³»ç»Ÿçš„æ•´ä½“åŠŸèƒ½æµç¨‹
    """

    if PROXY_URL: adata.proxy(is_proxy=True, proxy_url=PROXY_URL)

    st.title('A è‚¡æ•°æ®æ™ºèƒ½åˆ†æç³»ç»Ÿ')

    # æ˜¾ç¤ºæ ‡é¢˜å’Œä½œè€…ä¿¡æ¯
    st.markdown("""
    <div style='font-size: 0.9em; margin-bottom: 2em;'>
        <h3 style='color: #1f449c; font-size: 1.2em;'>
            ğŸ“š è¯¾ç¨‹ï¼šå•†ä¸šæ™ºèƒ½æŠ€æœ¯ 
            <small style='color: #666;'>/ æŒ‡å¯¼æ•™å¸ˆï¼šé˜®å…‰å†Œæ•™æˆ</small>
        </h3>
        <p style='color: #666; font-size: 0.9em; margin-top: 0.5em;'>
            ğŸ‘¨â€ğŸ“ å­¦ç”Ÿï¼šå´å°å®‡ 
            <span style='margin-left: 1em;'>ğŸ”¢ å­¦å·ï¼š71265700016</span>
            <span style='margin-left: 1em;'>â­ï¸ <a href='https://github.com/mantoufan/yzhanSimilarKline' target='_blank'>é¡¹ç›®æºç å’Œè¯´æ˜æ–‡æ¡£</a></span>
        </p>
    </div>
    """, unsafe_allow_html=True)

    # æ˜¾ç¤ºç³»ç»Ÿè¯´æ˜
    st.markdown("""
    <div style='background-color: #f8f9fa; padding: 1.2em; border-radius: 8px; 
        border-left: 4px solid #4c7ef3; margin-bottom: 2em; font-size: 0.9em;'>
        <h4 style='color: #1f449c; font-size: 1.1em; margin-bottom: 0.8em;'>
            ğŸ’¡ ç³»ç»ŸåŠŸèƒ½ä»‹ç»
        </h4>
        <p style='color: #444; line-height: 1.6; margin-bottom: 0.8em;'>
            æœ¬ç³»ç»Ÿç»“åˆäº†æŠ€æœ¯åˆ†æå’Œæ™ºèƒ½é—®ç­”åŠŸèƒ½ï¼š
        </p>
        <ul style='list-style-type: none; padding-left: 0.5em; margin: 0;'>
            <li style='margin-bottom: 0.5em;'>
                ğŸ“Š è‡ªåŠ¨è¯†åˆ«ç›¸ä¼¼Kçº¿å½¢æ€ (Kçº¿æ¨¡å¼è¯†åˆ« + æ¬§å‡ é‡Œå¾—è·ç¦» + çš®å°”é€Šç›¸å…³ç³»æ•° + æ—¶é—´åºåˆ—æ ‡å‡†åŒ–)
            </li>
            <li style='margin-bottom: 0.5em;'>
                ğŸ“ˆ é¢„æµ‹å¯èƒ½çš„ä»·æ ¼èµ°åŠ¿ (å†å²æ¨¡å¼åŒ¹é… + ç»Ÿè®¡æ¦‚ç‡åˆ†æ + è¶‹åŠ¿é¢„æµ‹å»ºæ¨¡)
            </li>
            <li style='margin-bottom: 0.5em;'>
                âš–ï¸ åˆ†æä¸åŒæŒä»“æœŸçš„é£é™©æ”¶ç›Š (æŒä»“æœŸæ”¶ç›Šç‡è®¡ç®— + é£é™©åº¦é‡ + èƒœç‡ç»Ÿè®¡)
            </li>
            <li style='margin-bottom: 0.5em;'>
                ğŸ¤– æä¾›æ™ºèƒ½é—®ç­”æœåŠ¡ (RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ + å‘é‡åŒ– + LLMé—®ç­” + ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…)
            </li>
        </ul>
    </div>
    """, unsafe_allow_html=True)
    
    # æœç´¢è¯åˆ¸
    query = st.text_input('è¾“å…¥æŒ‡æ•°/è‚¡ç¥¨/åŸºé‡‘ ETF çš„ä»£ç æˆ–åç§°è¿›è¡Œæœç´¢', '', key='security_search')
    
    if query:
        results = search_securities(query)
        
        if results:
            st.write(f'æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:')
            
            # æ˜¾ç¤ºæ¯ä¸ªæœç´¢ç»“æœ
            for result in results:
                with st.expander(f"{result['name']} ({result['code']}) - {result['type'].upper()}", expanded=False):
                    # æ˜¾ç¤ºè¯åˆ¸åŸºæœ¬ä¿¡æ¯
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.write(f"ğŸ“‡ ä»£ç : {result['code']}")
                    with col2:
                        st.write(f"ğŸ“ åç§°: {result['name']}")
                    with col3:
                        st.write(f"ğŸ¢ ç±»å‹: {result['type'].upper()}")
                    
                    if result['exchange']:
                        st.write(f"ğŸ›ï¸ äº¤æ˜“æ‰€: {result['exchange']}")
                    
                    # è·å–å¸‚åœºæ•°æ®
                    market_data = get_market_data(result['code'], result['type'])
                    
                    if market_data is not None:
                        # è·å–æœ€è¿‘ä¸€ä¸ªæœˆæ•°æ®ç”¨äºåˆ†æ
                        current_month_data = market_data.tail(30).copy()

                        # ç”Ÿæˆæœªæ¥7ä¸ªäº¤æ˜“æ—¥çš„æ—¥æœŸ
                        last_date = current_month_data['trade_date'].iloc[-1]
                        future_dates = pd.date_range(
                            start=last_date + pd.Timedelta(days=1),
                            periods=7,
                            freq='B'  # 'B' è¡¨ç¤ºå·¥ä½œæ—¥
                        )
                        
                        # å¯»æ‰¾ç›¸ä¼¼Kçº¿å½¢æ€
                        similar_patterns = find_similar_patterns(market_data, window_size=30, top_n=10)
                        
                        # æ˜¾ç¤ºå¸‚åœºåˆ†æ
                        holding_stats = display_market_analysis(current_month_data, similar_patterns, future_dates)
  
                        # æ˜¾ç¤ºæ™ºèƒ½é—®ç­”ç•Œé¢
                        if holding_stats is not None:
                            display_rag_qa(result, market_data.tail(252 * 10).copy(), similar_patterns, holding_stats)
                    else:
                        st.warning("æš‚æ— Kçº¿æ•°æ®")
        else:
            st.warning('æœªæ‰¾åˆ°åŒ¹é…çš„ç»“æœ')

def plot_kline(df, title, future_df=None, future_dates=None, show_future_data=False):
    """
    åˆ›å»ºKçº¿å›¾ï¼Œæ”¯æŒæ˜¾ç¤ºå†å²èµ°åŠ¿å’Œæœªæ¥é¢„æµ‹
    
    å‚æ•°è¯´æ˜ï¼š
    - df: å½“å‰Kçº¿æ•°æ®
    - title: å›¾è¡¨æ ‡é¢˜
    - future_df: æœªæ¥å®é™…æ•°æ®ï¼ˆç”¨äºå†å²ç›¸ä¼¼å›¾ï¼‰
    - future_dates: æœªæ¥æ—¥æœŸï¼ˆç”¨äºå½“å‰Kçº¿å›¾ï¼‰
    - show_future_data: æ˜¯å¦æ˜¾ç¤ºæœªæ¥æ•°æ®
    """
    if df is None or df.empty:
        return None
    
    fig = go.Figure()
    
    # æ·»åŠ ä¸»Kçº¿
    fig.add_trace(go.Candlestick(
        x=df['trade_date'],
        open=df['open'],
        high=df['high'],
        low=df['low'],
        close=df['close'],
        name='Kçº¿'
    ))
    
    # è·å–ä»·æ ¼èŒƒå›´
    y_range = [df['low'].min(), df['high'].max()]
    
    if future_df is not None and not future_df.empty:
        y_range = [
            min(y_range[0], future_df['low'].min()),
            max(y_range[1], future_df['high'].max())
        ]
    
    # æ·»åŠ åˆ†éš”çº¿
    last_date = df['trade_date'].max()
    fig.add_vline(
        x=last_date,
        line_width=1,
        line_dash="dash",
        line_color="gray",
        opacity=0.7
    )
    
    # å¤„ç†æœªæ¥æ•°æ®æ˜¾ç¤º
    if future_df is not None and not future_df.empty and show_future_data:
        fig.add_trace(go.Candlestick(
            x=future_df['trade_date'],
            open=future_df['open'],
            high=future_df['high'],
            low=future_df['low'],
            close=future_df['close'],
            name='åç»­èµ°åŠ¿'
        ))
    elif future_dates is not None:
        fig.add_trace(go.Scatter(
            x=future_dates,
            y=[None] * len(future_dates),
            mode='lines',
            line=dict(color='rgba(0,0,0,0)'),
            showlegend=False
        ))
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        height=350,
        title={
            'text': title,
            'y': 0.98,
            'x': 0.5,
            'xanchor': 'center',
            'yanchor': 'top'
        },
        showlegend=False,
        template='plotly_white',
        margin=dict(t=50, l=50, r=30, b=50),
        yaxis_title='ä»·æ ¼',
        xaxis_title='äº¤æ˜“æ—¥æœŸ',
        yaxis_range=y_range
    )
    
    # é…ç½®xè½´
    fig.update_xaxes(
        rangeslider=dict(visible=False),
        type='date',
        rangebreaks=[dict(bounds=["sat", "mon"])]
    )
    
    return fig

def display_trend_analysis(similar_patterns):
    """
    æ˜¾ç¤ºè¶‹åŠ¿åˆ†æç»“æœ
    """
    stats = analyze_future_trends(similar_patterns)
    
    if not stats:
        st.write("æœªæ‰¾åˆ°å†å²Kçº¿æ•°æ®")
        return
    
    st.markdown(f"### æœªæ¥äº¤æ˜“æ—¥æ¶¨è·Œé¢„æµ‹ï¼ˆåŸºäºæœ€ç›¸ä¼¼ {len(similar_patterns)} æ¡å†å² K çº¿ï¼‰")
    
    # åˆ›å»ºç½‘æ ¼å±•ç¤ºæ•°æ®
    html = create_trend_analysis_grid(stats)
    
    # æ˜¾ç¤ºç½‘æ ¼å¸ƒå±€
    st.markdown(html, unsafe_allow_html=True)
    
    # æ·»åŠ åˆ†æè¯´æ˜
    st.markdown("""
    <details class="details">
        <summary class="details-summary">
        ğŸ“Š <strong>æŒ‡æ ‡è®¡ç®—æ–¹æ³•è¯´æ˜</strong></summary>
        <div class="details-body">
            <p><strong>æ¶¨è·Œæ¦‚ç‡ï¼š</strong>å½“å¤©ä¸Šæ¶¨/ä¸‹è·Œçš„å†å²Kçº¿æ•°é‡ Ã· æ€»Kçº¿æ•°é‡</p>
            <p><strong>æ¶¨è·Œå¹…æŒ‡æ ‡ï¼ˆç›¸å¯¹äºå‰ä¸€å¤©æ”¶ç›˜ä»·ï¼‰ï¼š</strong></p>
            <ul>
                <li>æœ€é«˜/æœ€ä½æ¶¨è·Œå¹…ï¼šæå€¼æ¶¨è·Œå¹…</li>
                <li>å¹³å‡æ¶¨è·Œå¹…ï¼šæ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼</li>
                <li>æœ€å¯èƒ½æ¶¨è·Œå¹…ï¼šå‡ºç°é¢‘ç‡æœ€é«˜çš„æ¶¨è·Œå¹…</li>
            </ul>
            <p><strong>æ³¨æ„ï¼š</strong>æ‰€æœ‰è®¡ç®—å‡åŸºäºå†å²ç›¸ä¼¼Kçº¿çš„ç»Ÿè®¡ç»“æœï¼Œä»…ä¾›å‚è€ƒã€‚</p>
        </div>
    </details>
    """, unsafe_allow_html=True)

def create_trend_analysis_grid(stats):
    """
    åˆ›å»ºè¶‹åŠ¿åˆ†æç½‘æ ¼çš„HTMLå¸ƒå±€
    """
    metrics_data = {
        'up': [
            ('æ¶¨æ¦‚ç‡', 'probability', '%.1f%%'),
            ('æœ€é«˜æ¶¨å¹…', 'max', '%.2f%%'),
            ('å¹³å‡æ¶¨å¹…', 'mean', '%.2f%%'),
            ('æœ€å¯èƒ½æ¶¨å¹…', 'mode', '%.2f%%'),
            ('æœ€ä½æ¶¨å¹…', 'min', '%.2f%%')
        ],
        'down': [
            ('è·Œæ¦‚ç‡', 'probability', '%.1f%%'),
            ('æœ€é«˜è·Œå¹…', 'max', '%.2f%%'),
            ('å¹³å‡è·Œå¹…', 'mean', '%.2f%%'),
            ('æœ€å¯èƒ½è·Œå¹…', 'mode', '%.2f%%'),
            ('æœ€ä½è·Œå¹…', 'min', '%.2f%%')
        ]
    }
    
    # åˆ›å»ºHTMLå¸ƒå±€
    html = """
    <style>
        .grid-container {
            display: grid;
            grid-template-columns: 100px repeat(7, 1fr);
            gap: 1px;
            background-color: #ddd;
            padding: 1px;
            font-size: 12px;
            font-family: Arial, sans-serif;
        }
        .grid-header {
            background-color: #f4f4f4;
            padding: 8px;
            text-align: center;
            font-weight: bold;
        }
        .grid-label {
            background-color: #f4f4f4;
            padding: 8px;
            text-align: right;
        }
        .grid-cell {
            background-color: white;
            padding: 8px;
            text-align: center;
        }
        .up-value { color: #ff4444; }
        .down-value { color: #00cc00; }
        .details {
            background-color: rgb(240, 242, 246);
            padding: 15px;
            border-radius: 5px;
            margin-top: 16px;
            margin-bottom: 16px;
            transition: background-color 0.2s;
        }
        .details:hover {
            background-color: rgb(230, 232, 236);
        }
        .details-summary {
            margin: 0;
            font-size: 0.9em;
            color: #666;
            cursor: pointer;
        }
        .details-body {
            background-color: #f9f9f9;
            padding: 10px;
            margin: 5px 0;
            border-radius: 4px;
            font-family: monospace;
        }

        .details-body-related-chunks {
            font-size: 12px;
            white-space: pre-wrap;
        }

        .details-body-related-chunks p {
            background: #eee;
            padding: 20px;
        }

        .statement {
            background-color: #fff3cd;
            padding: 10px;
            border-radius: 5px; 
            font-size: 0.8em;
            margin-top: 10px;
            margin-bottom: 10px;
        }
    </style>
    <div class="grid-container">
        <div class="grid-header"></div>
    """
    
    # æ·»åŠ åˆ—æ ‡é¢˜
    for i in range(1, 8):
        html += f'<div class="grid-header">ç¬¬{i}æ—¥</div>'
    
    # å¤„ç†æ•°æ®è¡Œ
    for direction in ['up', 'down']:
        for label, metric_key, format_str in metrics_data[direction]:
            html += f'<div class="grid-label">{label}</div>'
            
            for day in range(1, 8):
                day_stats = stats[direction][str(day)]
                if day_stats['count'] > 0:
                    value = day_stats[metric_key]
                    if metric_key == 'probability':
                        value = value * 100
                    cell_content = format_str % value
                    style_class = 'up-value' if direction == 'up' else 'down-value'
                    html += f'<div class="grid-cell"><span class="{style_class}">{cell_content}</span></div>'
                else:
                    html += '<div class="grid-cell">æ— æ•°æ®</div>'
    
    html += "</div>"
    return html

def display_holding_period_analysis(stats, initial_money=10000):
    """
    æ˜¾ç¤ºä¸åŒæŒæœ‰æœŸçš„æ”¶ç›Šåˆ†æç»“æœ
    """
    if not stats:
        st.warning("æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®è¿›è¡Œåˆ†æ")
        return
        
    sample_count = list(stats.values())[0]['trade_count']
    
    st.markdown(f"### ç«‹å³ä¹°å…¥æ”¶ç›Šé¢„æµ‹ï¼ˆåŸºäºæœ€ç›¸ä¼¼çš„ {sample_count} æ¡å†å² K çº¿ï¼‰")
    
    # åˆ›å»ºè¡¨æ ¼æ•°æ®
    data = []
    for days, day_stats in stats.items():
        data.append({
            "æŒæœ‰æœŸ": f"{days}ä¸ªäº¤æ˜“æ—¥",
            "å¹³å‡æ”¶ç›Šç‡": f"{day_stats['avg_return']:.2f}%",
            "èƒœç‡": f"{day_stats['win_rate']*100:.1f}%",
            "æœ€å¤§ä¸Šæ¶¨": f"{day_stats['max_return']:.2f}%",
            "æœ€å¤§ä¸‹è·Œ": f"{day_stats['min_return']:.2f}%",
            "æ³¢åŠ¨ç‡": f"{day_stats['std_return']:.2f}%"
        })
    
    # æ˜¾ç¤ºè¡¨æ ¼
    st.dataframe(
        data,
        hide_index=True,
        use_container_width=True,
        column_config={
            "æŒæœ‰å¤©æ•°": st.column_config.TextColumn("æŒæœ‰å¤©æ•°", help="ä»å½“å‰äº¤æ˜“æ—¥æ”¶ç›˜ä»·ä¹°å…¥çš„æŒæœ‰æ—¶é—´"),
            "å¹³å‡æ”¶ç›Šç‡": st.column_config.TextColumn("å¹³å‡æ”¶ç›Šç‡", help="å†å²ç›¸ä¼¼æƒ…å†µä¸‹çš„å¹³å‡æ”¶ç›Šç‡"),
            "èƒœç‡": st.column_config.TextColumn("èƒœç‡", help="ç›ˆåˆ©æ¬¡æ•°/æ€»æ¬¡æ•°"),
            "æœ€å¤§ä¸Šæ¶¨": st.column_config.TextColumn("æœ€å¤§ä¸Šæ¶¨", help="å†å²ç›¸ä¼¼æƒ…å†µä¸‹çš„æœ€å¤§ä¸Šæ¶¨å¹…åº¦"),
            "æœ€å¤§ä¸‹è·Œ": st.column_config.TextColumn("æœ€å¤§ä¸‹è·Œ", help="å†å²ç›¸ä¼¼æƒ…å†µä¸‹çš„æœ€å¤§ä¸‹è·Œå¹…åº¦"),
            "æ³¢åŠ¨ç‡": st.column_config.TextColumn("æ³¢åŠ¨ç‡", help="æ”¶ç›Šç‡çš„æ ‡å‡†å·®,åæ˜ é£é™©å¤§å°")
        }
    )
    
    # æ·»åŠ æŒ‡æ ‡è®¡ç®—æ–¹æ³•è¯´æ˜
    st.markdown("""
    <details class="details" style="margin-top: -12px;">
        <summary class="details-summary">
        ğŸ“Š <strong>æŒ‡æ ‡è®¡ç®—æ–¹æ³•è¯´æ˜</strong></summary >
        <div class="details-body">
            <p><strong>æ”¶ç›Šç‡å’Œé£é™©æŒ‡æ ‡è®¡ç®—å…¬å¼ï¼š</strong></p>
            <ul>
                <li><strong>å¹³å‡æ”¶ç›Šç‡</strong> = (å„æ ·æœ¬çš„æ”¶ç›Šç‡ä¹‹å’Œ) Ã· æ ·æœ¬æ•°é‡<br>
                    å…¶ä¸­ï¼Œå•ä¸ªæ ·æœ¬æ”¶ç›Šç‡ = (å–å‡ºä»·æ ¼ - ä¹°å…¥ä»·æ ¼) Ã· ä¹°å…¥ä»·æ ¼ Ã— 100%</li>
                <li><strong>èƒœç‡</strong> = ç›ˆåˆ©æ¬¡æ•° Ã· æ€»äº¤æ˜“æ¬¡æ•° Ã— 100%<br>
                    å½“æ”¶ç›Šç‡ > 0 æ—¶ï¼Œè®¡ä¸ºç›ˆåˆ©ï¼›å½“æ”¶ç›Šç‡ â‰¤ 0 æ—¶ï¼Œè®¡ä¸ºäºæŸ</li>
                <li><strong>æœ€å¤§ä¸Šæ¶¨</strong> = MAX((å–å‡ºä»·æ ¼ - ä¹°å…¥ä»·æ ¼) Ã· ä¹°å…¥ä»·æ ¼ Ã— 100%)<br>
                    åœ¨æ‰€æœ‰æ ·æœ¬ä¸­ï¼Œé€‰å–æ”¶ç›Šç‡æœ€é«˜çš„è®°å½•</li>
                <li><strong>æœ€å¤§ä¸‹è·Œ</strong> = MIN((å–å‡ºä»·æ ¼ - ä¹°å…¥ä»·æ ¼) Ã· ä¹°å…¥ä»·æ ¼ Ã— 100%)<br>
                    åœ¨æ‰€æœ‰æ ·æœ¬ä¸­ï¼Œé€‰å–æ”¶ç›Šç‡æœ€ä½çš„è®°å½•</li>
                <li><strong>æ³¢åŠ¨ç‡</strong> = STDEV(æ‰€æœ‰æ ·æœ¬æ”¶ç›Šç‡) = âˆš(âˆ‘(æ”¶ç›Šç‡ - å¹³å‡æ”¶ç›Šç‡)Â² Ã· (æ ·æœ¬æ•°é‡-1))<br>
                    åæ˜ æ”¶ç›Šç‡çš„ç¦»æ•£ç¨‹åº¦ï¼Œæ³¢åŠ¨ç‡è¶Šå¤§ä»£è¡¨é£é™©è¶Šé«˜</li>
            </ul>
            <p><strong>é‡è¦æç¤ºï¼š</strong></p>
            <ul>
                <li>ä¹°å…¥ä»·æ ¼ä¸ºå½“å‰Kçº¿çš„æ”¶ç›˜ä»·</li>
                <li>å–å‡ºä»·æ ¼ä¸ºæŒæœ‰æœŸç»“æŸæ—¶çš„æ”¶ç›˜ä»·</li>
                <li>æ‰€æœ‰ç»Ÿè®¡å‡åŸºäºå†å²ç›¸ä¼¼Kçº¿æ•°æ®ï¼Œä»…ä¾›å‚è€ƒ</li>
                <li>è¿‡å¾€è¡¨ç°ä¸ä»£è¡¨æœªæ¥æ”¶ç›Šï¼ŒæŠ•èµ„éœ€è°¨æ…</li>
            </ul>
        </div>
    </details>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()