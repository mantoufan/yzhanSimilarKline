import streamlit as st
import requests
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from datetime import datetime, timedelta
from scipy.stats import pearsonr
from scipy.spatial.distance import euclidean
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import jieba
import re
import adata
import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# API é…ç½® - ä¼˜å…ˆä» .env è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä» streamlit ç¯å¢ƒå˜é‡è¯»å–
API_KEY = os.getenv('API_KEY') or st.secrets.get("API_KEY")
API_BASE = os.getenv('API_BASE') or st.secrets.get("API_BASE", "https://api.openai.com")
MODEL = os.getenv('MODEL') or st.secrets.get("MODEL", "gpt-4o-mini")
PROXY_URL = os.getenv('PROXY_URL') or st.secrets.get("PROXY_URL")

def search_securities(query):
    """æœç´¢è¯åˆ¸(æŒ‡æ•°ã€è‚¡ç¥¨ã€åŸºé‡‘ã€å€ºåˆ¸)"""
    results = []
    
    # æœç´¢æŒ‡æ•°
    try:
        indices_df = adata.stock.info.all_index_code()
        indices = indices_df[
            (indices_df['index_code'].str.contains(query, case=False)) |
            (indices_df['name'].str.contains(query, case=False))
        ]
        for _, row in indices.iterrows():
            results.append({
                'code': row['index_code'],
                'name': row['name'],
                'type': 'index',
                'exchange': ''
            })
    except:
        pass

    # æœç´¢è‚¡ç¥¨
    try:
        stocks_df = adata.stock.info.all_code()
        stocks = stocks_df[
            (stocks_df['stock_code'].str.contains(query, case=False)) |
            (stocks_df['short_name'].str.contains(query, case=False))
        ]
        for _, row in stocks.iterrows():
            results.append({
                'code': row['stock_code'],
                'name': row['short_name'],
                'type': 'stock',
                'exchange': row['exchange']
            })
    except:
        pass

    # æœç´¢ETF
    try:
        etfs_df = adata.fund.info.all_etf_exchange_traded_info()
        etfs = etfs_df[
            (etfs_df['fund_code'].str.contains(query, case=False)) |
            (etfs_df['short_name'].str.contains(query, case=False))
        ]
        for _, row in etfs.iterrows():
            results.append({
                'code': row['fund_code'],
                'name': row['short_name'],
                'type': 'etf',
                'exchange': ''
            })
    except:
        pass

    return results

def get_market_data(code, security_type, days=365*3):
    """è·å–å¸‚åœºæ•°æ®"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    df = None
    
    if security_type == 'stock':
        df = adata.stock.market.get_market(
            stock_code=code,
            start_date=start_date.strftime('%Y-%m-%d'),
            k_type=1,
            adjust_type=1
        )
    elif security_type == 'etf':
        df = adata.fund.market.get_market_etf(
            fund_code=code,
            k_type=1
        )
    elif security_type == 'index':
        df = adata.stock.market.get_market_index(
            index_code=code,
            start_date=start_date.strftime('%Y-%m-%d'),
            k_type=1
        )
    
    if df is not None and not df.empty:
        df['trade_date'] = pd.to_datetime(df['trade_date'])
        numeric_columns = ['open', 'high', 'low', 'close']
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=numeric_columns)
        return df
    
    return None

def normalize_window(window):
    """æ ‡å‡†åŒ–ä»·æ ¼åºåˆ—"""
    numeric_window = pd.to_numeric(window, errors='coerce')
    if numeric_window.isna().any():
        return None
    return (numeric_window - numeric_window.iloc[0]) / numeric_window.iloc[0] * 100

def calculate_similarity(window1, window2):
    """è®¡ç®—ä¸¤ä¸ªçª—å£ä¹‹é—´çš„ç›¸ä¼¼åº¦"""
    if len(window1) != len(window2):
        return 0
    
    norm1 = normalize_window(window1)
    norm2 = normalize_window(window2)
    
    if norm1 is None or norm2 is None:
        return 0
    
    try:
        corr, _ = pearsonr(norm1, norm2)
        dist = euclidean(norm1, norm2)
        normalized_dist = 1 / (1 + dist/len(window1))
        similarity = (corr + 1)/2 * 0.7 + normalized_dist * 0.3
        return similarity
    except:
        return 0

def find_similar_patterns(df, window_size=30, top_n=3):
    """
    æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å†å²æ¨¡å¼
    
    æŠ€æœ¯ç‚¹ï¼š
    - Kçº¿æ¨¡å¼è¯†åˆ«ï¼šé€šè¿‡æ»‘åŠ¨çª—å£æ–¹æ³•è¯†åˆ«å†å²Kçº¿å½¢æ€
    - æ¬§å‡ é‡Œå¾—è·ç¦»ï¼šè®¡ç®—Kçº¿åºåˆ—é—´çš„è·ç¦»ç›¸ä¼¼åº¦
    - çš®å°”é€Šç›¸å…³ç³»æ•°ï¼šè®¡ç®—èµ°åŠ¿çš„ç›¸å…³æ€§
    - æ—¶é—´åºåˆ—æ ‡å‡†åŒ–ï¼šå°†ä¸åŒæ—¶æœŸçš„ä»·æ ¼åºåˆ—æ ‡å‡†åŒ–ä»¥ä¾¿æ¯”è¾ƒ
    """
    if df is None or len(df) < window_size * 2:
        return []
    
    recent_window = df.tail(window_size)['close']
    similar_patterns = []
    max_i = len(df) - (window_size * 2 + 7)
    
    for i in range(max_i):
        historical_window = df.iloc[i:i+window_size]['close']
        future_data = df.iloc[i+window_size:i+window_size+7]
        
        if len(future_data) < 7:
            continue
            
        similarity = calculate_similarity(recent_window, historical_window)
        
        if similarity > 0:
            similar_patterns.append({
                'start_date': df.iloc[i]['trade_date'],
                'end_date': df.iloc[i+window_size-1]['trade_date'],
                'pattern_data': df.iloc[i:i+window_size],
                'future_data': future_data,
                'similarity': similarity
            })
    
    similar_patterns.sort(key=lambda x: x['similarity'], reverse=True)
    return similar_patterns[:top_n]

def analyze_future_trends(similar_patterns):
    """
    åˆ†æå†å²Kçº¿åç»­èµ°åŠ¿
    
    æŠ€æœ¯ç‚¹ï¼š
    - å†å²æ¨¡å¼åŒ¹é…ï¼šåŸºäºç›¸ä¼¼Kçº¿æ¨¡å¼çš„å†å²è¡¨ç°
    - ç»Ÿè®¡æ¦‚ç‡åˆ†æï¼šè®¡ç®—æ¶¨è·Œæ¦‚ç‡å’Œå¹…åº¦åˆ†å¸ƒ
    - è¶‹åŠ¿é¢„æµ‹å»ºæ¨¡ï¼šæ„å»ºæœªæ¥å¯èƒ½çš„èµ°åŠ¿é¢„æµ‹
    """
    if not similar_patterns:
        return None
        
    stats = {
        'up': {str(i): {'count': 0, 'max': 0, 'min': float('inf'), 'mean': 0, 'values': []} 
               for i in range(1, 8)},
        'down': {str(i): {'count': 0, 'max': 0, 'min': float('inf'), 'mean': 0, 'values': []} 
                 for i in range(1, 8)}
    }
    
    for pattern in similar_patterns:
        future_data = pattern['future_data']
        
        for i in range(len(future_data)):
            day = str(i + 1)
            current_price = future_data.iloc[i]['close']
            prev_price = pattern['pattern_data'].iloc[-1]['close'] if i == 0 else future_data.iloc[i-1]['close']
            
            change_rate = ((current_price - prev_price) / prev_price) * 100
            
            category = 'up' if change_rate >= 0 else 'down'
            change_rate = abs(change_rate)
            
            stats[category][day]['count'] += 1
            stats[category][day]['values'].append(change_rate)
            stats[category][day]['max'] = max(stats[category][day]['max'], change_rate)
            stats[category][day]['min'] = min(stats[category][day]['min'], change_rate)
    
    total_patterns = len(similar_patterns)
    for category in ['up', 'down']:
        for day in stats[category]:
            day_stats = stats[category][day]
            if day_stats['count'] > 0:
                day_stats['probability'] = day_stats['count'] / total_patterns
                day_stats['mean'] = sum(day_stats['values']) / day_stats['count']
                rounded_values = [round(x, 2) for x in day_stats['values']]
                value_counts = {}
                for v in rounded_values:
                    value_counts[v] = value_counts.get(v, 0) + 1
                day_stats['mode'] = max(value_counts.items(), key=lambda x: x[1])[0]
                
                if day_stats['min'] == float('inf'):
                    day_stats['min'] = 0
                
            del day_stats['values']
    
    return stats

def analyze_holding_returns(similar_patterns):
    """
    åˆ†æä¸åŒæŒæœ‰æœŸçš„æ”¶ç›Šæƒ…å†µ
    
    æŠ€æœ¯ç‚¹ï¼š
    - æŒä»“æœŸæ”¶ç›Šç‡è®¡ç®—ï¼šè®¡ç®—ä¸åŒæŒæœ‰æœŸçš„æ”¶ç›Šè¡¨ç°
    - é£é™©åº¦é‡(æ ‡å‡†å·®/æ³¢åŠ¨ç‡)ï¼šè¯„ä¼°æŠ•èµ„é£é™©æ°´å¹³
    - èƒœç‡ç»Ÿè®¡ï¼šåˆ†æä¸åŒæŒæœ‰æœŸçš„ç›ˆåˆ©æ¦‚ç‡
    """
    if not similar_patterns:
        return None
        
    stats = {str(i): {
        'returns': [],
        'max_prices': [],
        'min_prices': [],
        'win_count': 0,
        'loss_count': 0,
    } for i in range(1, 8)}
    
    for pattern in similar_patterns:
        entry_price = pattern['pattern_data'].iloc[-1]['close']
        future_data = pattern['future_data']
        
        for days in range(1, 8):
            day_key = str(days)
            if days <= len(future_data):
                holding_period_data = future_data.iloc[:days]
                exit_price = holding_period_data.iloc[-1]['close']
                returns = (exit_price - entry_price) / entry_price * 100
                
                stats[day_key]['returns'].append(returns)
                
                if returns > 0:
                    stats[day_key]['win_count'] += 1
                else:
                    stats[day_key]['loss_count'] += 1
                
                stats[day_key]['max_prices'].append(holding_period_data['high'].max())
                stats[day_key]['min_prices'].append(holding_period_data['low'].min())
    
    analysis_results = {}
    for days, day_stats in stats.items():
        returns_array = np.array(day_stats['returns'])
        total_trades = len(returns_array)
        
        if total_trades > 0:
            analysis_results[days] = {
                'avg_return': np.mean(returns_array),
                'max_return': np.max(returns_array),
                'min_return': np.min(returns_array),
                'std_return': np.std(returns_array),
                'win_rate': day_stats['win_count'] / total_trades,
                'trade_count': total_trades,
                'max_price_change': (np.max(day_stats['max_prices']) - entry_price) / entry_price * 100,
                'min_price_change': (np.min(day_stats['min_prices']) - entry_price) / entry_price * 100
            }
    
    return analysis_results

class ChineseTextVectorizer:
    """ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–å™¨"""
    
    def __init__(self, vector_size=100):
        self.vector_size = vector_size
        self.tfidf = TfidfVectorizer(
            tokenizer=self._tokenize,
            token_pattern=None,
            max_features=5000
        )
        self.svd = TruncatedSVD(
            n_components=vector_size,
            random_state=42
        )
        self.is_fitted = False
    
    def _tokenize(self, text):
        text = re.sub(r'[^\w\s]', '', text)
        words = jieba.cut(text)
        return [w for w in words if w.strip()]
    
    def fit(self, texts):
        tfidf_matrix = self.tfidf.fit_transform(texts)
        self.svd.fit(tfidf_matrix)
        self.is_fitted = True
    
    def transform(self, text):
        tfidf_vector = self.tfidf.transform([text])
        vector = self.svd.transform(tfidf_vector)
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        return vector.flatten()

def call_llm_api(prompt):
    """è°ƒç”¨ LLM API"""
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }
    
    try:
        response = requests.post(
            f"{API_BASE}/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        st.error(f"API è°ƒç”¨å‡ºé”™: {str(e)}")
        return None

def compute_embedding(text, vectorizer=None):
    """è®¡ç®—æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
    if vectorizer is None:
        vectorizer = ChineseTextVectorizer()
        
    try:
        if not text.strip():
            return np.zeros(vectorizer.vector_size)
            
        if not vectorizer.is_fitted:
            vectorizer.fit([text])
            
        embedding = vectorizer.transform(text)
        return embedding
        
    except Exception as e:
        print(f"è®¡ç®—æ–‡æœ¬å‘é‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return np.zeros(vectorizer.vector_size)

def compute_similarity(query_embedding, chunk_embedding):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if query_embedding.shape != chunk_embedding.shape:
        raise ValueError("å‘é‡ç»´åº¦ä¸åŒ¹é…")
        
    similarity = np.dot(query_embedding, chunk_embedding)
    return similarity

def retrieve_relevant_chunks(query, chunks, top_k=3):
    """æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æœ¬å—"""
    vectorizer = ChineseTextVectorizer()
    all_texts = [query] + [chunk_text for _, chunk_text in chunks]
    vectorizer.fit(all_texts)
    
    query_embedding = compute_embedding(query, vectorizer)
    
    similarities = []
    for chunk_id, chunk_text in chunks:
        chunk_embedding = compute_embedding(chunk_text, vectorizer)
        similarity = compute_similarity(query_embedding, chunk_embedding)
        similarities.append((similarity, chunk_id, chunk_text))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰ top_k ä¸ª
    similarities.sort(reverse=True)
    return [(chunk_id, chunk_text) for _, chunk_id, chunk_text in similarities[:top_k]]

def create_text_chunks(security, current_df, similar_patterns, holding_stats):
    """
    å°†å¸‚åœºæ•°æ®åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„æ–‡æœ¬å—ï¼Œä¾¿äºåç»­æ£€ç´¢

    æŠ€æœ¯ç‚¹ï¼š
    - RAGæ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæ„å»ºç»“æ„åŒ–çš„æ–‡æœ¬å—ä¾›æ£€ç´¢
    - å‘é‡åŒ–(TF-IDF + SVD)ï¼šå°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡è¡¨ç¤º
    - ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…ï¼šè®¡ç®—æŸ¥è¯¢ä¸æ–‡æœ¬å—çš„ç›¸ä¼¼åº¦
    
    è¿™ä¸ªå‡½æ•°å°†å„ç±»å¸‚åœºæ•°æ®è½¬æ¢æˆç»“æ„åŒ–çš„æ–‡æœ¬å—ï¼ŒåŒ…æ‹¬ï¼š
    1. è¯åˆ¸åŸºæœ¬ä¿¡æ¯ï¼ˆä»£ç ã€åç§°ç­‰ï¼‰
    2. æœ€æ–°è¡Œæƒ…æ•°æ®
    3. è¿‘æœŸä»·æ ¼èµ°åŠ¿
    4. å†å²è¡¨ç°åˆ†æ
    5. ç›¸ä¼¼Kçº¿åˆ†æ
    6. æŒä»“æ”¶ç›Šåˆ†æ
    """
    chunks = []
    
    # æ„å»ºåŸºæœ¬ä¿¡æ¯æ–‡æœ¬å—
    basic_info = f"""
            è¯åˆ¸åŸºæœ¬ä¿¡æ¯ï¼š
            åç§°ï¼š{security['name']}
            ä»£ç ï¼š{security['code']}
            ç±»å‹ï¼š{security['type']}
            äº¤æ˜“æ‰€ï¼š{security['exchange'] if security['exchange'] else 'æœªçŸ¥'}
    """
    chunks.append(("basic_info", basic_info))
    
    if current_df is not None and not current_df.empty:
        # æ·»åŠ æœ€æ–°è¡Œæƒ…ä¿¡æ¯
        latest_data = current_df.iloc[-1]
        latest_market = f"""
        æœ€æ–°å¸‚åœºè¡Œæƒ…ï¼ˆ{latest_data['trade_date'].strftime('%Y-%m-%d')}ï¼‰ï¼š
        æ”¶ç›˜ä»·ï¼š{latest_data['close']:.2f}
        å¼€ç›˜ä»·ï¼š{latest_data['open']:.2f}
        æœ€é«˜ä»·ï¼š{latest_data['high']:.2f}
        æœ€ä½ä»·ï¼š{latest_data['low']:.2f}
        æˆäº¤é‡ï¼š{latest_data.get('volume', 'æœªçŸ¥')}
        """
        chunks.append(("latest_market", latest_market))
        
        # æ·»åŠ è¿‘æœŸèµ°åŠ¿åˆ†æï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼‰
        if len(current_df) >= 21:
            recent_trend = f"""
            è¿‘æœŸä»·æ ¼èµ°åŠ¿ï¼š
            5æ—¥æ¶¨è·Œå¹…ï¼š{((latest_data['close'] - current_df.iloc[-6]['close'])/current_df.iloc[-6]['close']*100):.2f}%
            10æ—¥æ¶¨è·Œå¹…ï¼š{((latest_data['close'] - current_df.iloc[-11]['close'])/current_df.iloc[-11]['close']*100):.2f}%
            20æ—¥æ¶¨è·Œå¹…ï¼š{((latest_data['close'] - current_df.iloc[-21]['close'])/current_df.iloc[-21]['close']*100):.2f}%
            """
            chunks.append(("recent_trend", recent_trend))
        
        # æ·»åŠ å†å²è¡¨ç°åˆ†æï¼ˆæœ€è¿‘3å¹´ï¼‰
        three_year_data = current_df.tail(252 * 3).copy()
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col in three_year_data.columns:
                three_year_data[col] = pd.to_numeric(three_year_data[col], errors='coerce')
        
        if not three_year_data.empty:
            # æŒ‰å¹´åº¦ç»Ÿè®¡å¸‚åœºè¡¨ç°
            yearly_data = {}
            for year in three_year_data['trade_date'].dt.year.unique():
                year_df = three_year_data[three_year_data['trade_date'].dt.year == year]
                if not year_df.empty:
                    yearly_data[year] = {
                        'start_price': year_df.iloc[0]['close'],
                        'end_price': year_df.iloc[-1]['close'],
                        'highest': year_df['high'].max(),
                        'lowest': year_df['low'].min(),
                        'vol_mean': pd.to_numeric(year_df.get('volume', pd.Series()), errors='coerce').mean()
                    }
            
            # æ„å»ºå¹´åº¦åˆ†ææ–‡æœ¬
            historical_trend = """
            è¿‘å¹´å¸‚åœºè¡¨ç°ï¼š"""
            for year, data in sorted(yearly_data.items(), reverse=True):
                yearly_return = ((data['end_price'] - data['start_price']) / data['start_price'] * 100)
                yearly_volatility = ((data['highest'] - data['lowest']) / data['lowest'] * 100)
                
                historical_trend += f"""
                {year}å¹´åº¦è¡¨ç°ï¼š
                å¹´åº¦æ¶¨è·Œå¹…ï¼š{yearly_return:.2f}%
                æœ€é«˜ä»·ï¼š{data['highest']:.2f}
                æœ€ä½ä»·ï¼š{data['lowest']:.2f}
                æ³¢åŠ¨ç‡ï¼š{yearly_volatility:.2f}%"""
                if pd.notnull(data['vol_mean']):
                    historical_trend += f"""å¹³å‡æˆäº¤é‡ï¼š{data['vol_mean']:.0f}"""
            
            # æ·»åŠ 3å¹´æ•´ä½“ç»Ÿè®¡
            total_return = ((latest_data['close'] - three_year_data.iloc[0]['close']) 
                          / three_year_data.iloc[0]['close'] * 100)
            max_price = three_year_data['high'].max()
            min_price = three_year_data['low'].min()
            total_volatility = ((max_price - min_price) / min_price * 100)
            
            historical_trend += f"""
            è¿‘å¹´æ•´ä½“ç»Ÿè®¡ï¼š
            ç´¯è®¡æ¶¨è·Œå¹…ï¼š{total_return:.2f}%
            å†å²æœ€é«˜ä»·ï¼š{max_price:.2f}
            å†å²æœ€ä½ä»·ï¼š{min_price:.2f}
            ä»·æ ¼æ³¢åŠ¨ç‡ï¼š{total_volatility:.2f}%
            """
            
            chunks.append(("historical_trend", historical_trend))
    
    # æ·»åŠ ç›¸ä¼¼Kçº¿åˆ†æç»“æœ
    if similar_patterns:
        for i, pattern in enumerate(similar_patterns[:3], 1):
            future_trend = ""
            for j, row in pattern['future_data'].iterrows():
                future_trend += f"""
            ç¬¬{j+1}æ—¥:{row["close"]:.2f}"""
            
            pattern_info = f"""
            å†å²ç›¸ä¼¼Kçº¿åˆ†æï¼ˆTOP {i}ï¼‰ï¼š
            ç›¸ä¼¼åº¦ï¼š{pattern['similarity']:.2%}
            å†å²æ—¶é—´æ®µï¼š{pattern['start_date'].strftime('%Y-%m-%d')} è‡³ {pattern['end_date'].strftime('%Y-%m-%d')}
            åç»­7æ—¥å®é™…èµ°åŠ¿ï¼š{future_trend}
                """
            chunks.append((f"similar_pattern_{i}", pattern_info))
    
    # æ·»åŠ æŒä»“åˆ†ææ•°æ®
    if holding_stats:
        for days, stats in holding_stats.items():
            holding_info = f"""            æŒä»“{days}å¤©åˆ†æï¼š
            å¹³å‡æ”¶ç›Šï¼š{stats['avg_return']:.2f}%
            èƒœç‡ï¼š{stats['win_rate']*100:.1f}%
            æœ€å¤§ä¸Šæ¶¨ï¼š{stats['max_return']:.2f}%
            æœ€å¤§ä¸‹è·Œï¼š{stats['min_return']:.2f}%
            æ³¢åŠ¨ç‡ï¼š{stats['std_return']:.2f}%"""
            chunks.append((f"holding_days_{days}", holding_info))
    
    return chunks

def get_analysis_prompt(query, relevant_chunks):
    """
    æ„å»ºå¸¦æœ‰æ£€ç´¢ä¸Šä¸‹æ–‡çš„åˆ†ææç¤º
    
    å°†ç›¸å…³çš„å¸‚åœºæ•°æ®æ•´åˆåˆ°æç¤ºä¸­ï¼Œå¼•å¯¼æ¨¡å‹åŸºäºå®é™…æ•°æ®è¿›è¡Œåˆ†æï¼Œ
    åŒæ—¶æé†’æ³¨æ„æŠ•èµ„é£é™©ã€‚
    """
    context = "\n".join([chunk for _, chunk in relevant_chunks])
    
    prompt = f"""ä½œä¸ºä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹ç›¸å…³å¸‚åœºæ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. åªä½¿ç”¨æä¾›çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦æ·»åŠ å…¶ä»–å¸‚åœºä¿¡æ¯
2. æ˜ç¡®åŒºåˆ†æ•°æ®æ”¯æŒçš„ç»“è®ºå’Œä¸ç¡®å®šçš„æ¨æµ‹
3. å¦‚æœæ•°æ®ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
4. é€‚å½“æé†’æŠ•èµ„é£é™©

ç›¸å…³å¸‚åœºæ•°æ®ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}
"""
    return prompt

def display_market_analysis(current_df, similar_patterns):
    """
    å¸‚åœºåˆ†æä¸»å‡½æ•°ï¼Œé›†æˆKçº¿å±•ç¤ºã€æŠ€æœ¯åˆ†æå’Œæ™ºèƒ½é—®ç­”åŠŸèƒ½
    """
    if current_df is None or current_df.empty:
        st.warning("æ— æ³•è·å–å¸‚åœºæ•°æ®")
        return None
        
    # æ˜¾ç¤ºå½“å‰Kçº¿å›¾
    with st.container():
        st.markdown("### å½“å‰Kçº¿å›¾")
        if not current_df.empty:
            fig = plot_kline(current_df, "")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.write("æ— æ³•ç”ŸæˆKçº¿å›¾")
    
    # åˆ†æå¹¶æ˜¾ç¤ºç›¸ä¼¼Kçº¿
    if similar_patterns: 
        st.markdown("### å†å²ç›¸ä¼¼Kçº¿å›¾ï¼ˆå±•ç¤ºç›¸ä¼¼åº¦æœ€é«˜çš„å‰3æ¡ï¼‰")
        
        # æ˜¾ç¤ºæ¯ä¸ªç›¸ä¼¼æ¨¡å¼
        for i, pattern in enumerate(similar_patterns[:3], 1):
            with st.container():
                st.markdown(f"#### TOP {i} ç›¸ä¼¼åº¦: {pattern['similarity']:.2%}")
                st.markdown(f"å†å²æ—¶é—´æ®µ: {pattern['start_date'].strftime('%Y-%m-%d')} - {pattern['end_date'].strftime('%Y-%m-%d')}")
                
                fig_similar = plot_kline(
                    pattern['pattern_data'],
                    "",
                    future_df=pattern['future_data'],
                    show_future_data=True
                )
                st.plotly_chart(fig_similar, use_container_width=True)
        
        # æ˜¾ç¤ºè¶‹åŠ¿åˆ†æ
        display_trend_analysis(similar_patterns)

        # è®¡ç®—å’Œæ˜¾ç¤ºæŒä»“æ”¶ç›Šåˆ†æ
        holding_stats = analyze_holding_returns(similar_patterns)
        display_holding_period_analysis(holding_stats)
        
        return holding_stats
    
    return None

def display_rag_qa(security, current_df, similar_patterns, holding_stats):
    """
    æ˜¾ç¤ºæ™ºèƒ½é—®ç­”ç•Œé¢
    
    æ•´åˆRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯ï¼Œä¸ºç”¨æˆ·æä¾›åŸºäºå®é™…å¸‚åœºæ•°æ®çš„æ™ºèƒ½é—®ç­”æœåŠ¡ã€‚
    """
    st.markdown("### æ™ºèƒ½é—®ç­”åŠ©æ‰‹")

    # åˆ›å»ºæ–‡æœ¬å—ç”¨äºæ£€ç´¢
    chunks = create_text_chunks(security, current_df, similar_patterns, holding_stats)
    base_key = f"{security['code']}_{security['type']}"
    
    # ç”¨æˆ·è¾“å…¥é—®é¢˜
    user_question = st.text_input(
        f"è¯·è¾“å…¥æ‚¨å…³äº {security['name']}ï¼ˆ{security['code']}ï¼‰çš„é—®é¢˜ï¼š", 
        key=f'rag_question_{base_key}',
        help="æ‚¨å¯ä»¥è¯¢é—®å…³äºè¯¥è¯åˆ¸çš„åŸºæœ¬ä¿¡æ¯ã€æœ€æ–°è¡Œæƒ…ã€å†å²è¡¨ç°ã€ç›¸ä¼¼Kçº¿åˆ†æç­‰é—®é¢˜"
    )
    
    if user_question:
        with st.spinner('æ­£åœ¨æ€è€ƒç­”æ¡ˆ...'):
            # æ£€ç´¢ç›¸å…³æ–‡æœ¬å—
            relevant_chunks = retrieve_relevant_chunks(user_question, chunks, top_k=10)
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æœ¬å—
            # é¦–å…ˆæ„å»ºæ•°æ®å—çš„å†…å®¹
            chunks_content = ""
            for _, chunk_text in relevant_chunks:
                # æ³¨æ„ï¼šç§»é™¤äº†å¤šä½™çš„ç¼©è¿›å’Œæ¢è¡Œ
                chunks_content += f'<p>{chunk_text}</p>'

            # æ³¨æ„ï¼šä¿æŒ HTML ç»“æ„ç´§å‡‘ï¼Œé¿å…ä¸å¿…è¦çš„ç¼©è¿›å’Œæ¢è¡Œ
            st.markdown(f'<details class="details" style="margin-top: -6px;">'
                    f'<summary class="details-summary">ğŸ” ç›¸å…³æ•°æ®å—ï¼ˆåŸºäº RAG ç»“æœï¼‰</summary>'
                    f'<div class="details-body details-body-related-chunks">'
                    f'{chunks_content}'
                    f'</div>'
                    f'</details>', unsafe_allow_html=True)
            
            # æ„å»ºpromptå¹¶è°ƒç”¨API
            prompt = get_analysis_prompt(user_question, relevant_chunks)
            response = call_llm_api(prompt)
            
            if response:
                # æ˜¾ç¤ºåˆ†æç»“æœ
                st.markdown("#### åˆ†æå›ç­”ï¼š")
                st.markdown(response)
                
                # æ·»åŠ å…è´£å£°æ˜
                st.markdown("""
                <div class="statement">
                âš ï¸ <strong>å…è´£å£°æ˜ï¼š</strong>ä»¥ä¸Šåˆ†æä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚
                </div>
                """, unsafe_allow_html=True)
            else:
                st.error("æŠ±æ­‰ï¼Œè·å–å›ç­”å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚")

def main():
    if PROXY_URL: adata.proxy(is_proxy=True, proxy_url=PROXY_URL)
    
    """
    ä¸»å‡½æ•°ï¼šå®ç°é‡‘èæ•°æ®åˆ†æç³»ç»Ÿçš„æ•´ä½“åŠŸèƒ½æµç¨‹
    """
    st.title('A è‚¡æ•°æ®æ™ºèƒ½åˆ†æç³»ç»Ÿ')

    # æ˜¾ç¤ºæ ‡é¢˜å’Œä½œè€…ä¿¡æ¯
    st.markdown("""
    <div style='font-size: 0.9em; margin-bottom: 2em;'>
        <h3 style='color: #1f449c; font-size: 1.2em;'>
            ğŸ“š è¯¾ç¨‹ï¼šå•†ä¸šæ™ºèƒ½æŠ€æœ¯ 
            <small style='color: #666;'>/ æŒ‡å¯¼æ•™å¸ˆï¼šé˜®å…‰å†Œæ•™æˆ</small>
        </h3>
        <p style='color: #666; font-size: 0.9em; margin-top: 0.5em;'>
            ğŸ‘¨â€ğŸ“ å­¦ç”Ÿï¼šå´å°å®‡ 
            <span style='margin-left: 1em;'>ğŸ”¢ å­¦å·ï¼š71265700016</span>
        </p>
    </div>
    """, unsafe_allow_html=True)

    # æ˜¾ç¤ºç³»ç»Ÿè¯´æ˜
    st.markdown("""
    <div style='background-color: #f8f9fa; padding: 1.2em; border-radius: 8px; 
        border-left: 4px solid #4c7ef3; margin-bottom: 2em; font-size: 0.9em;'>
        <h4 style='color: #1f449c; font-size: 1.1em; margin-bottom: 0.8em;'>
            ğŸ’¡ ç³»ç»ŸåŠŸèƒ½ä»‹ç»
        </h4>
        <p style='color: #444; line-height: 1.6; margin-bottom: 0.8em;'>
            æœ¬ç³»ç»Ÿç»“åˆäº†æŠ€æœ¯åˆ†æå’Œæ™ºèƒ½é—®ç­”åŠŸèƒ½ï¼š
        </p>
        <ul style='list-style-type: none; padding-left: 0.5em; margin: 0;'>
            <li style='margin-bottom: 0.5em;'>
                ğŸ“Š è‡ªåŠ¨è¯†åˆ«ç›¸ä¼¼Kçº¿å½¢æ€ (Kçº¿æ¨¡å¼è¯†åˆ« + æ¬§å‡ é‡Œå¾—è·ç¦» + çš®å°”é€Šç›¸å…³ç³»æ•° + æ—¶é—´åºåˆ—æ ‡å‡†åŒ–)
            </li>
            <li style='margin-bottom: 0.5em;'>
                ğŸ“ˆ é¢„æµ‹å¯èƒ½çš„ä»·æ ¼èµ°åŠ¿ (å†å²æ¨¡å¼åŒ¹é… + ç»Ÿè®¡æ¦‚ç‡åˆ†æ + è¶‹åŠ¿é¢„æµ‹å»ºæ¨¡)
            </li>
            <li style='margin-bottom: 0.5em;'>
                âš–ï¸ åˆ†æä¸åŒæŒä»“æœŸçš„é£é™©æ”¶ç›Š (æŒä»“æœŸæ”¶ç›Šç‡è®¡ç®— + é£é™©åº¦é‡ + èƒœç‡ç»Ÿè®¡)
            </li>
            <li style='margin-bottom: 0.5em;'>
                ğŸ¤– æä¾›æ™ºèƒ½é—®ç­”æœåŠ¡ (RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ + å‘é‡åŒ– + LLMé—®ç­” + ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…)
            </li>
        </ul>
    </div>
    """, unsafe_allow_html=True)
    
    # æœç´¢è¯åˆ¸
    query = st.text_input('è¾“å…¥æŒ‡æ•°/è‚¡ç¥¨/åŸºé‡‘çš„ä»£ç æˆ–åç§°è¿›è¡Œæœç´¢', '', key='security_search')
    
    if query:
        results = search_securities(query)
        
        if results:
            st.write(f'æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:')
            
            # æ˜¾ç¤ºæ¯ä¸ªæœç´¢ç»“æœ
            for result in results:
                with st.expander(f"{result['name']} ({result['code']}) - {result['type'].upper()}", expanded=False):
                    # æ˜¾ç¤ºè¯åˆ¸åŸºæœ¬ä¿¡æ¯
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.write(f"ğŸ“‡ ä»£ç : {result['code']}")
                    with col2:
                        st.write(f"ğŸ“ åç§°: {result['name']}")
                    with col3:
                        st.write(f"ğŸ¢ ç±»å‹: {result['type'].upper()}")
                    
                    if result['exchange']:
                        st.write(f"ğŸ›ï¸ äº¤æ˜“æ‰€: {result['exchange']}")
                    
                    # è·å–å¸‚åœºæ•°æ®
                    market_data = get_market_data(result['code'], result['type'])
                    
                    if market_data is not None:
                        # è·å–æœ€è¿‘ä¸€ä¸ªæœˆæ•°æ®ç”¨äºåˆ†æ
                        current_month_data = market_data.tail(30).copy()
                        
                        # å¯»æ‰¾ç›¸ä¼¼Kçº¿å½¢æ€
                        similar_patterns = find_similar_patterns(market_data, window_size=30, top_n=10)
                        
                        # æ˜¾ç¤ºå¸‚åœºåˆ†æ
                        holding_stats = display_market_analysis(current_month_data, similar_patterns)
  
                        # æ˜¾ç¤ºæ™ºèƒ½é—®ç­”ç•Œé¢
                        if holding_stats is not None:
                            display_rag_qa(result, market_data.tail(252 * 10).copy(), similar_patterns, holding_stats)
                    else:
                        st.warning("æš‚æ— Kçº¿æ•°æ®")
        else:
            st.warning('æœªæ‰¾åˆ°åŒ¹é…çš„ç»“æœ')

def plot_kline(df, title, future_df=None, future_dates=None, show_future_data=False):
    """
    åˆ›å»ºKçº¿å›¾ï¼Œæ”¯æŒæ˜¾ç¤ºå†å²èµ°åŠ¿å’Œæœªæ¥é¢„æµ‹
    
    å‚æ•°è¯´æ˜ï¼š
    - df: å½“å‰Kçº¿æ•°æ®
    - title: å›¾è¡¨æ ‡é¢˜
    - future_df: æœªæ¥å®é™…æ•°æ®ï¼ˆç”¨äºå†å²ç›¸ä¼¼å›¾ï¼‰
    - future_dates: æœªæ¥æ—¥æœŸï¼ˆç”¨äºå½“å‰Kçº¿å›¾ï¼‰
    - show_future_data: æ˜¯å¦æ˜¾ç¤ºæœªæ¥æ•°æ®
    """
    if df is None or df.empty:
        return None
    
    fig = go.Figure()
    
    # æ·»åŠ ä¸»Kçº¿
    fig.add_trace(go.Candlestick(
        x=df['trade_date'],
        open=df['open'],
        high=df['high'],
        low=df['low'],
        close=df['close'],
        name='Kçº¿'
    ))
    
    # è·å–ä»·æ ¼èŒƒå›´
    y_range = [df['low'].min(), df['high'].max()]
    
    if future_df is not None and not future_df.empty:
        y_range = [
            min(y_range[0], future_df['low'].min()),
            max(y_range[1], future_df['high'].max())
        ]
    
    # æ·»åŠ åˆ†éš”çº¿
    last_date = df['trade_date'].max()
    fig.add_vline(
        x=last_date,
        line_width=1,
        line_dash="dash",
        line_color="gray",
        opacity=0.7
    )
    
    # å¤„ç†æœªæ¥æ•°æ®æ˜¾ç¤º
    if future_df is not None and not future_df.empty and show_future_data:
        fig.add_trace(go.Candlestick(
            x=future_df['trade_date'],
            open=future_df['open'],
            high=future_df['high'],
            low=future_df['low'],
            close=future_df['close'],
            name='åç»­èµ°åŠ¿'
        ))
    elif future_dates is not None:
        fig.add_trace(go.Scatter(
            x=future_dates,
            y=[None] * len(future_dates),
            mode='lines',
            line=dict(color='rgba(0,0,0,0)'),
            showlegend=False
        ))
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        height=350,
        title={
            'text': title,
            'y': 0.98,
            'x': 0.5,
            'xanchor': 'center',
            'yanchor': 'top'
        },
        showlegend=False,
        template='plotly_white',
        margin=dict(t=50, l=50, r=30, b=50),
        yaxis_title='ä»·æ ¼',
        xaxis_title='äº¤æ˜“æ—¥æœŸ',
        yaxis_range=y_range
    )
    
    # é…ç½®xè½´
    fig.update_xaxes(
        rangeslider=dict(visible=False),
        type='date',
        rangebreaks=[dict(bounds=["sat", "mon"])]
    )
    
    return fig

def display_trend_analysis(similar_patterns):
    """
    æ˜¾ç¤ºè¶‹åŠ¿åˆ†æç»“æœ
    """
    stats = analyze_future_trends(similar_patterns)
    
    if not stats:
        st.write("æœªæ‰¾åˆ°å†å²Kçº¿æ•°æ®")
        return
    
    st.markdown(f"### æœªæ¥äº¤æ˜“æ—¥æ¶¨è·Œé¢„æµ‹ï¼ˆåŸºäºæœ€ç›¸ä¼¼ {len(similar_patterns)} æ¡å†å² K çº¿ï¼‰")
    
    # åˆ›å»ºç½‘æ ¼å±•ç¤ºæ•°æ®
    html = create_trend_analysis_grid(stats)
    
    # æ˜¾ç¤ºç½‘æ ¼å¸ƒå±€
    st.markdown(html, unsafe_allow_html=True)
    
    # æ·»åŠ åˆ†æè¯´æ˜
    st.markdown("""
    <details class="details">
        <summary class="details-summary">
        ğŸ“Š <strong>æŒ‡æ ‡è®¡ç®—æ–¹æ³•è¯´æ˜</strong></summary>
        <div class="details-body">
            <p><strong>æ¶¨è·Œæ¦‚ç‡ï¼š</strong>å½“å¤©ä¸Šæ¶¨/ä¸‹è·Œçš„å†å²Kçº¿æ•°é‡ Ã· æ€»Kçº¿æ•°é‡</p>
            <p><strong>æ¶¨è·Œå¹…æŒ‡æ ‡ï¼ˆç›¸å¯¹äºå‰ä¸€å¤©æ”¶ç›˜ä»·ï¼‰ï¼š</strong></p>
            <ul>
                <li>æœ€é«˜/æœ€ä½æ¶¨è·Œå¹…ï¼šæå€¼æ¶¨è·Œå¹…</li>
                <li>å¹³å‡æ¶¨è·Œå¹…ï¼šæ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼</li>
                <li>æœ€å¯èƒ½æ¶¨è·Œå¹…ï¼šå‡ºç°é¢‘ç‡æœ€é«˜çš„æ¶¨è·Œå¹…</li>
            </ul>
            <p><strong>æ³¨æ„ï¼š</strong>æ‰€æœ‰è®¡ç®—å‡åŸºäºå†å²ç›¸ä¼¼Kçº¿çš„ç»Ÿè®¡ç»“æœï¼Œä»…ä¾›å‚è€ƒã€‚</p>
        </div>
    </details>
    """, unsafe_allow_html=True)

def create_trend_analysis_grid(stats):
    """
    åˆ›å»ºè¶‹åŠ¿åˆ†æç½‘æ ¼çš„HTMLå¸ƒå±€
    """
    metrics_data = {
        'up': [
            ('æ¶¨æ¦‚ç‡', 'probability', '%.1f%%'),
            ('æœ€é«˜æ¶¨å¹…', 'max', '%.2f%%'),
            ('å¹³å‡æ¶¨å¹…', 'mean', '%.2f%%'),
            ('æœ€å¯èƒ½æ¶¨å¹…', 'mode', '%.2f%%'),
            ('æœ€ä½æ¶¨å¹…', 'min', '%.2f%%')
        ],
        'down': [
            ('è·Œæ¦‚ç‡', 'probability', '%.1f%%'),
            ('æœ€é«˜è·Œå¹…', 'max', '%.2f%%'),
            ('å¹³å‡è·Œå¹…', 'mean', '%.2f%%'),
            ('æœ€å¯èƒ½è·Œå¹…', 'mode', '%.2f%%'),
            ('æœ€ä½è·Œå¹…', 'min', '%.2f%%')
        ]
    }
    
    # åˆ›å»ºHTMLå¸ƒå±€
    html = """
    <style>
        .grid-container {
            display: grid;
            grid-template-columns: 100px repeat(7, 1fr);
            gap: 1px;
            background-color: #ddd;
            padding: 1px;
            font-size: 12px;
            font-family: Arial, sans-serif;
        }
        .grid-header {
            background-color: #f4f4f4;
            padding: 8px;
            text-align: center;
            font-weight: bold;
        }
        .grid-label {
            background-color: #f4f4f4;
            padding: 8px;
            text-align: right;
        }
        .grid-cell {
            background-color: white;
            padding: 8px;
            text-align: center;
        }
        .up-value { color: #ff4444; }
        .down-value { color: #00cc00; }
        .details {
            background-color: rgb(240, 242, 246);
            padding: 15px;
            border-radius: 5px;
            margin-top: 16px;
            margin-bottom: 16px;
            transition: background-color 0.2s;
        }
        .details:hover {
            background-color: rgb(230, 232, 236);
        }
        .details-summary {
            margin: 0;
            font-size: 0.9em;
            color: #666;
            cursor: pointer;
        }
        .details-body {
            font-size: 12px;
            background-color: #f9f9f9;
            padding: 10px;
            margin: 5px 0;
            border-radius: 4px;
            white-space: pre-wrap;
            font-family: monospace;
        }

        .details-body-related-chunks p {
            background: #eee;
            padding: 20px;
        }

        .statement {
            background-color: #fff3cd;
            padding: 10px;
            border-radius: 5px; 
            font-size: 0.8em;
            margin-top: 10px;
            margin-bottom: 10px;
        }
    </style>
    <div class="grid-container">
        <div class="grid-header"></div>
    """
    
    # æ·»åŠ åˆ—æ ‡é¢˜
    for i in range(1, 8):
        html += f'<div class="grid-header">ç¬¬{i}æ—¥</div>'
    
    # å¤„ç†æ•°æ®è¡Œ
    for direction in ['up', 'down']:
        for label, metric_key, format_str in metrics_data[direction]:
            html += f'<div class="grid-label">{label}</div>'
            
            for day in range(1, 8):
                day_stats = stats[direction][str(day)]
                if day_stats['count'] > 0:
                    value = day_stats[metric_key]
                    if metric_key == 'probability':
                        value = value * 100
                    cell_content = format_str % value
                    style_class = 'up-value' if direction == 'up' else 'down-value'
                    html += f'<div class="grid-cell"><span class="{style_class}">{cell_content}</span></div>'
                else:
                    html += '<div class="grid-cell">æ— æ•°æ®</div>'
    
    html += "</div>"
    return html

def display_holding_period_analysis(stats, initial_money=10000):
    """
    æ˜¾ç¤ºä¸åŒæŒæœ‰æœŸçš„æ”¶ç›Šåˆ†æç»“æœ
    """
    if not stats:
        st.warning("æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®è¿›è¡Œåˆ†æ")
        return
        
    sample_count = list(stats.values())[0]['trade_count']
    
    st.markdown(f"### ç«‹å³ä¹°å…¥æ”¶ç›Šé¢„æµ‹ï¼ˆåŸºäºæœ€ç›¸ä¼¼çš„ {sample_count} æ¡å†å² K çº¿ï¼‰")
    
    # åˆ›å»ºè¡¨æ ¼æ•°æ®
    data = []
    for days, day_stats in stats.items():
        data.append({
            "æŒæœ‰æœŸ": f"{days}ä¸ªäº¤æ˜“æ—¥",
            "å¹³å‡æ”¶ç›Šç‡": f"{day_stats['avg_return']:.2f}%",
            "èƒœç‡": f"{day_stats['win_rate']*100:.1f}%",
            "æœ€å¤§ä¸Šæ¶¨": f"{day_stats['max_return']:.2f}%",
            "æœ€å¤§ä¸‹è·Œ": f"{day_stats['min_return']:.2f}%",
            "æ³¢åŠ¨ç‡": f"{day_stats['std_return']:.2f}%"
        })
    
    # æ˜¾ç¤ºè¡¨æ ¼
    st.dataframe(
        data,
        hide_index=True,
        use_container_width=True,
        column_config={
            "æŒæœ‰å¤©æ•°": st.column_config.TextColumn("æŒæœ‰å¤©æ•°", help="ä»å½“å‰äº¤æ˜“æ—¥æ”¶ç›˜ä»·ä¹°å…¥çš„æŒæœ‰æ—¶é—´"),
            "å¹³å‡æ”¶ç›Šç‡": st.column_config.TextColumn("å¹³å‡æ”¶ç›Šç‡", help="å†å²ç›¸ä¼¼æƒ…å†µä¸‹çš„å¹³å‡æ”¶ç›Šç‡"),
            "èƒœç‡": st.column_config.TextColumn("èƒœç‡", help="ç›ˆåˆ©æ¬¡æ•°/æ€»æ¬¡æ•°"),
            "æœ€å¤§ä¸Šæ¶¨": st.column_config.TextColumn("æœ€å¤§ä¸Šæ¶¨", help="å†å²ç›¸ä¼¼æƒ…å†µä¸‹çš„æœ€å¤§ä¸Šæ¶¨å¹…åº¦"),
            "æœ€å¤§ä¸‹è·Œ": st.column_config.TextColumn("æœ€å¤§ä¸‹è·Œ", help="å†å²ç›¸ä¼¼æƒ…å†µä¸‹çš„æœ€å¤§ä¸‹è·Œå¹…åº¦"),
            "æ³¢åŠ¨ç‡": st.column_config.TextColumn("æ³¢åŠ¨ç‡", help="æ”¶ç›Šç‡çš„æ ‡å‡†å·®,åæ˜ é£é™©å¤§å°")
        }
    )
    
    # æ·»åŠ æŒ‡æ ‡è®¡ç®—æ–¹æ³•è¯´æ˜
    st.markdown("""
    <details class="details" style="margin-top: -12px;">
        <summary class="details-summary">
        ğŸ“Š <strong>æŒ‡æ ‡è®¡ç®—æ–¹æ³•è¯´æ˜</strong></summary >
        <div class="details-body">
            <p><strong>æ”¶ç›Šç‡å’Œé£é™©æŒ‡æ ‡è®¡ç®—å…¬å¼ï¼š</strong></p>
            <ul>
                <li><strong>å¹³å‡æ”¶ç›Šç‡</strong> = (å„æ ·æœ¬çš„æ”¶ç›Šç‡ä¹‹å’Œ) Ã· æ ·æœ¬æ•°é‡<br>
                    å…¶ä¸­ï¼Œå•ä¸ªæ ·æœ¬æ”¶ç›Šç‡ = (å–å‡ºä»·æ ¼ - ä¹°å…¥ä»·æ ¼) Ã· ä¹°å…¥ä»·æ ¼ Ã— 100%</li>
                <li><strong>èƒœç‡</strong> = ç›ˆåˆ©æ¬¡æ•° Ã· æ€»äº¤æ˜“æ¬¡æ•° Ã— 100%<br>
                    å½“æ”¶ç›Šç‡ > 0 æ—¶ï¼Œè®¡ä¸ºç›ˆåˆ©ï¼›å½“æ”¶ç›Šç‡ â‰¤ 0 æ—¶ï¼Œè®¡ä¸ºäºæŸ</li>
                <li><strong>æœ€å¤§ä¸Šæ¶¨</strong> = MAX((å–å‡ºä»·æ ¼ - ä¹°å…¥ä»·æ ¼) Ã· ä¹°å…¥ä»·æ ¼ Ã— 100%)<br>
                    åœ¨æ‰€æœ‰æ ·æœ¬ä¸­ï¼Œé€‰å–æ”¶ç›Šç‡æœ€é«˜çš„è®°å½•</li>
                <li><strong>æœ€å¤§ä¸‹è·Œ</strong> = MIN((å–å‡ºä»·æ ¼ - ä¹°å…¥ä»·æ ¼) Ã· ä¹°å…¥ä»·æ ¼ Ã— 100%)<br>
                    åœ¨æ‰€æœ‰æ ·æœ¬ä¸­ï¼Œé€‰å–æ”¶ç›Šç‡æœ€ä½çš„è®°å½•</li>
                <li><strong>æ³¢åŠ¨ç‡</strong> = STDEV(æ‰€æœ‰æ ·æœ¬æ”¶ç›Šç‡) = âˆš(âˆ‘(æ”¶ç›Šç‡ - å¹³å‡æ”¶ç›Šç‡)Â² Ã· (æ ·æœ¬æ•°é‡-1))<br>
                    åæ˜ æ”¶ç›Šç‡çš„ç¦»æ•£ç¨‹åº¦ï¼Œæ³¢åŠ¨ç‡è¶Šå¤§ä»£è¡¨é£é™©è¶Šé«˜</li>
            </ul>
            <p><strong>é‡è¦æç¤ºï¼š</strong></p>
            <ul>
                <li>ä¹°å…¥ä»·æ ¼ä¸ºå½“å‰Kçº¿çš„æ”¶ç›˜ä»·</li>
                <li>å–å‡ºä»·æ ¼ä¸ºæŒæœ‰æœŸç»“æŸæ—¶çš„æ”¶ç›˜ä»·</li>
                <li>æ‰€æœ‰ç»Ÿè®¡å‡åŸºäºå†å²ç›¸ä¼¼Kçº¿æ•°æ®ï¼Œä»…ä¾›å‚è€ƒ</li>
                <li>è¿‡å¾€è¡¨ç°ä¸ä»£è¡¨æœªæ¥æ”¶ç›Šï¼ŒæŠ•èµ„éœ€è°¨æ…</li>
            </ul>
        </div>
    </details>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()